#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
×§×•×‘×¥ ×”×’×“×¨×•×ª ××¢×¨×›×ª RAG - ××›×œ×œ×ª ××¤×§×”
========================================

×§×•×‘×¥ ×–×” ××›×™×œ ××ª ×›×œ ×”×’×“×¨×•×ª ××¢×¨×›×ª RAG ×‘××§×•× ××¨×›×–×™ ××—×“.
×©×™× ×•×™ ×¢×¨×›×™× ×›××Ÿ ×™×©×¤×™×¢ ×¢×œ ×›×œ ×”××¢×¨×›×ª ×‘××•×¤×Ÿ ××—×™×“.

×œ×¦×•×¨×š ××•×¤×˜×™××™×–×¦×™×”, ×¢×¨×•×š ××ª ×”×¢×¨×›×™× ×›××Ÿ ×•×¨×¥ ×‘×“×™×§×•×ª ×‘×™×¦×•×¢×™×.
"""

import os
from typing import Dict, Any, List
from dataclasses import dataclass, field


@dataclass
class SearchConfig:
    """×”×’×“×¨×•×ª ×—×™×¤×•×© ×‘××¢×¨×›×ª RAG"""
    
    # ×¡×¤×™ ×“××™×•×Ÿ ×œ×—×™×¤×•×©
    SIMILARITY_THRESHOLD: float = 0.3  # ×¡×£ ×“××™×•×Ÿ ××™× ×™××œ×™ ×œ×›×œ ×”×¦'×× ×§×™× (lowered for better results)
    HIGH_QUALITY_THRESHOLD: float = 0.75  # ×¡×£ ×œ×ª×•×¦××•×ª ××™×›×•×ª×™×•×ª ×’×‘×•×”×•×ª
    LOW_QUALITY_THRESHOLD: float = 0.40  # ×¡×£ ××™× ×™××œ×™ ×œ×ª×•×¦××•×ª ×—×œ×©×•×ª
    
    # ××¡×¤×¨ ×ª×•×¦××•×ª ××§×¡×™××œ×™
    MAX_CHUNKS_RETRIEVED: int = 12  # ××¡×¤×¨ ×¦'×× ×§×™× ××§×¡×™××œ×™ ×œ×›×œ ×—×™×¤×•×©
    MAX_CHUNKS_FOR_CONTEXT: int = 8  # ××¡×¤×¨ ×¦'×× ×§×™× ×‘×¤×•×¢×œ ×œ×©×œ×™×—×” ×œ-LLM
    MAX_RESULTS_EXTENDED: int = 20  # ×—×™×¤×•×© ××•×¨×—×‘ ×œ×¦×¨×›×™× ××™×•×—×“×™×
    
    # ××©×§×œ×™× ×œ×—×™×¤×•×© ×”×™×‘×¨×™×“×™
    HYBRID_SEMANTIC_WEIGHT: float = 0.7  # ××©×§×œ ×”×—×™×¤×•×© ×”×¡×× ×˜×™
    HYBRID_KEYWORD_WEIGHT: float = 0.3  # ××©×§×œ ×—×™×¤×•×© ××™×œ×•×ª ×”××¤×ª×—
    
    # ×”×’×“×¨×•×ª ×–××Ÿ ×ª×’×•×‘×”
    SEARCH_TIMEOUT_SECONDS: int = 30  # ×–××Ÿ ×”××ª× ×” ××§×¡×™××œ×™ ×œ×—×™×¤×•×©
    EMBEDDING_TIMEOUT_SECONDS: int = 15  # ×–××Ÿ ×”××ª× ×” ×œ×™×¦×™×¨×ª embedding


@dataclass
class EmbeddingConfig:
    """×”×’×“×¨×•×ª ×™×¦×™×¨×ª embeddings"""
    
    # ××•×“×œ ×”-embedding
    MODEL_NAME: str = "models/embedding-001"  # ××•×“×œ Gemini ×œembedding
    TASK_TYPE_DOCUMENT: str = "retrieval_document"  # ×¡×•×’ ××©×™××” ×œ××¡××›×™×
    TASK_TYPE_QUERY: str = "retrieval_query"  # ×¡×•×’ ××©×™××” ×œ×©××™×œ×ª×•×ª
    
    # ×”×’×“×¨×•×ª embedding
    EMBEDDING_DIMENSION: int = 768  # ×××“ ×”-embedding (×œ××•×“×œ Gemini)
    BATCH_SIZE: int = 10  # ××¡×¤×¨ ×˜×§×¡×˜×™× ×œ×¢×™×‘×•×“ ×‘×‘×ª ××—×ª
    MAX_INPUT_LENGTH: int = 8192  # ××•×¨×š ××§×¡×™××œ×™ ×©×œ ×˜×§×¡×˜ ×œembedding
    
    # ×”×’×“×¨×•×ª retry
    MAX_RETRIES: int = 3  # ××¡×¤×¨ × ×™×¡×™×•× ×•×ª ×—×•×–×¨×™×
    RETRY_DELAY_SECONDS: float = 1.0  # ×–××Ÿ ×”××ª× ×” ×‘×™×Ÿ × ×™×¡×™×•× ×•×ª


@dataclass
class ChunkConfig:
    """×”×’×“×¨×•×ª ×¢×™×‘×•×“ ×•×—×œ×•×§×ª ×¦'×× ×§×™×"""
    
    # ×’×•×“×œ ×¦'×× ×§×™×
    DEFAULT_CHUNK_SIZE: int = 2000  # ×’×•×“×œ ×¦'×× ×§ ×‘×ª×•×•×™×
    MIN_CHUNK_SIZE: int = 100  # ×’×•×“×œ ××™× ×™××œ×™ ×œ×¦'×× ×§
    MAX_CHUNK_SIZE: int = 4000  # ×’×•×“×œ ××§×¡×™××œ×™ ×œ×¦'×× ×§
    
    # ×—×¤×™×¤×” ×‘×™×Ÿ ×¦'×× ×§×™×
    DEFAULT_CHUNK_OVERLAP: int = 200  # ×—×¤×™×¤×” ×¨×’×™×œ×” ×‘×ª×•×•×™×
    MIN_CHUNK_OVERLAP: int = 50  # ×—×¤×™×¤×” ××™× ×™××œ×™×ª
    MAX_CHUNK_OVERLAP: int = 500  # ×—×¤×™×¤×” ××§×¡×™××œ×™×ª
    
    # ×”×’×“×¨×•×ª ×¢×™×‘×•×“ ××¡××›×™×
    MAX_CHUNKS_PER_DOCUMENT: int = 500  # ××¡×¤×¨ ×¦'×× ×§×™× ××§×¡×™××œ×™ ×œ××¡××š
    MIN_CHUNKS_PER_DOCUMENT: int = 1  # ××¡×¤×¨ ×¦'×× ×§×™× ××™× ×™××œ×™
    
    # ×”×’×“×¨×•×ª ×˜×•×§× ×™×
    MAX_TOKENS_PER_CHUNK: int = 512  # ×˜×•×§× ×™× ××§×¡×™××œ×™ ×œ×¦'×× ×§
    TARGET_TOKENS_PER_CHUNK: int = 350  # ×™×¢×“ ×˜×•×§× ×™× ×œ×¦'×× ×§
    MIN_TOKENS_PER_CHUNK: int = 50  # ×˜×•×§× ×™× ××™× ×™××œ×™ ×œ×¦'×× ×§


@dataclass
class ContextConfig:
    """×”×’×“×¨×•×ª ×‘× ×™×™×ª ×”×§×©×¨ ×œ-LLM"""
    
    # ×”×’×“×¨×•×ª ×˜×•×§× ×™×
    MAX_CONTEXT_TOKENS: int = 6000  # ××§×¡×™××•× ×˜×•×§× ×™× ×œ×›×œ ×”×”×§×©×¨
    RESERVED_TOKENS_FOR_QUERY: int = 500  # ×˜×•×§× ×™× ×©××•×¨×™× ×œ×©××™×œ×ª×”
    RESERVED_TOKENS_FOR_RESPONSE: int = 1500  # ×˜×•×§× ×™× ×©××•×¨×™× ×œ×ª×©×•×‘×”
    
    # ×—×™×©×•×‘ ×–××™×Ÿ ×œ×”×§×©×¨
    @property
    def AVAILABLE_CONTEXT_TOKENS(self) -> int:
        return (self.MAX_CONTEXT_TOKENS - 
                self.RESERVED_TOKENS_FOR_QUERY - 
                self.RESERVED_TOKENS_FOR_RESPONSE)
    
    # ×”×’×“×¨×•×ª ×‘× ×™×™×ª ×”×§×©×¨
    INCLUDE_CITATIONS: bool = True  # ×”×× ×œ×›×œ×•×œ ×¦×™×˜×•×˜×™×
    INCLUDE_CHUNK_HEADERS: bool = True  # ×”×× ×œ×›×œ×•×œ ×›×•×ª×¨×•×ª ×¦'×× ×§×™×
    INCLUDE_PAGE_NUMBERS: bool = True  # ×”×× ×œ×›×œ×•×œ ××¡×¤×¨×™ ×¢××•×“×™×
    
    # ××¤×¨×™×“×™×
    CHUNK_SEPARATOR: str = "\n\n---\n\n"  # ××¤×¨×™×“ ×‘×™×Ÿ ×¦'×× ×§×™×
    CITATION_SEPARATOR: str = " | "  # ××¤×¨×™×“ ×‘×¦×™×˜×•×˜×™×


@dataclass
class LLMConfig:
    """×”×’×“×¨×•×ª ××•×“×œ ×”×©×¤×”"""
    
    # ××•×“×œ ×”-LLM
    MODEL_NAME: str = "gemini-2.0-flash"
    TEMPERATURE: float = 0.1  # × ××•×š ×œ×ª×©×•×‘×•×ª ×¢×§×‘×™×•×ª
    MAX_OUTPUT_TOKENS: int = 2048  # ××•×¨×š ×ª×©×•×‘×” ××§×¡×™××œ×™
    
    # ×”×’×“×¨×•×ª ×‘×˜×™×—×•×ª
    SAFETY_SETTINGS: Dict[str, str] = field(default_factory=lambda: {
        "HARM_CATEGORY_HARASSMENT": "BLOCK_NONE",
        "HARM_CATEGORY_HATE_SPEECH": "BLOCK_NONE", 
        "HARM_CATEGORY_SEXUALLY_EXPLICIT": "BLOCK_NONE",
        "HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_NONE"
    })
    
    # ×”×’×“×¨×•×ª timeout
    GENERATION_TIMEOUT_SECONDS: int = 45  # ×–××Ÿ ×”××ª× ×” ×œ×™×¦×™×¨×ª ×ª×©×•×‘×”


@dataclass
class DatabaseConfig:
    """×”×’×“×¨×•×ª ××¡×“ × ×ª×•× ×™×"""
    
    # ×©××•×ª ×˜×‘×œ××•×ª
    DOCUMENTS_TABLE: str = "documents"
    CHUNKS_TABLE: str = "document_chunks"
    ANALYTICS_TABLE: str = "search_analytics"
    
    # ×©××•×ª ×¤×•× ×§×¦×™×•×ª RPC
    SEMANTIC_SEARCH_FUNCTION: str = "match_documents_semantic"  # Changed to working function
    HYBRID_SEARCH_FUNCTION: str = "hybrid_search_documents"
    CONTEXTUAL_SEARCH_FUNCTION: str = "contextual_search"
    ANALYTICS_FUNCTION: str = "log_search_analytics"
    LOG_ANALYTICS_FUNCTION: str = "log_search_analytics"  # Added missing field
    
    # ×”×’×“×¨×•×ª Connection Pool
    MAX_CONNECTIONS: int = 20
    CONNECTION_TIMEOUT: int = 30


@dataclass
class PerformanceConfig:
    """×”×’×“×¨×•×ª ×‘×™×¦×•×¢×™×"""
    
    # ×–×× ×™ ×ª×’×•×‘×” ××§×¡×™××œ×™×™× (×‘××™×œ×™×©× ×™×•×ª)
    MAX_SEARCH_TIME_MS: int = 5000  # 5 ×©× ×™×•×ª
    MAX_EMBEDDING_TIME_MS: int = 3000  # 3 ×©× ×™×•×ª
    MAX_GENERATION_TIME_MS: int = 8000  # 8 ×©× ×™×•×ª
    
    # ×–×× ×™ ×ª×’×•×‘×” ×™×¢×“ (×‘××™×œ×™×©× ×™×•×ª)
    TARGET_SEARCH_TIME_MS: int = 2000  # 2 ×©× ×™×•×ª
    TARGET_EMBEDDING_TIME_MS: int = 1000  # 1 ×©× ×™×™×”
    TARGET_GENERATION_TIME_MS: int = 4000  # 4 ×©× ×™×•×ª
    
    # ×”×’×“×¨×•×ª caching
    ENABLE_EMBEDDING_CACHE: bool = True
    EMBEDDING_CACHE_SIZE: int = 1000  # ××¡×¤×¨ embeddings ×‘××˜××•×Ÿ
    EMBEDDING_CACHE_TTL_SECONDS: int = 3600  # ×©×¢×”
    
    # ×”×’×“×¨×•×ª ×œ×•×’×™× ×•× ×™×ª×•×—
    LOG_SEARCH_ANALYTICS: bool = True  # ×¨×™×©×•× ×¡×˜×˜×™×¡×˜×™×§×•×ª ×—×™×¤×•×©
    LOG_PERFORMANCE_METRICS: bool = True  # ×¨×™×©×•× ××“×“×™ ×‘×™×¦×•×¢×™×


@dataclass
class OptimizationConfig:
    """×”×’×“×¨×•×ª ××•×¤×˜×™××™×–×¦×™×”"""
    
    # ×”×’×“×¨×•×ª ×œ×‘×“×™×§×•×ª A/B
    ENABLE_AB_TESTING: bool = False
    AB_TEST_SPLIT_RATIO: float = 0.5  # 50/50 split
    
    # ×”×’×“×¨×•×ª ×œ×•×’×™× ×•× ×™×ª×•×—
    ENABLE_DETAILED_LOGGING: bool = True
    LOG_SEARCH_ANALYTICS: bool = True
    LOG_PERFORMANCE_METRICS: bool = True
    
    # ×”×’×“×¨×•×ª ××•×¤×˜×™××™×–×¦×™×” ××•×˜×•××˜×™×ª
    AUTO_ADJUST_THRESHOLDS: bool = False  # ××•×¤×˜×™××™×–×¦×™×” ××•×˜×•××˜×™×ª ×©×œ ×¡×¤×™×
    MIN_SEARCH_RESULTS_FOR_ADJUSTMENT: int = 100  # ××™× ×™××•× ×—×™×¤×•×©×™× ×œ××•×¤×˜×™××™×–×¦×™×”


class RAGConfig:
    """××—×œ×§×” ×¨××©×™×ª ×œ×”×’×“×¨×•×ª RAG"""
    
    def __init__(self):
        self.search = SearchConfig()
        self.embedding = EmbeddingConfig()
        self.chunk = ChunkConfig()
        self.context = ContextConfig()
        self.llm = LLMConfig()
        self.database = DatabaseConfig()
        self.performance = PerformanceConfig()
        self.optimization = OptimizationConfig()
    
    def get_config_dict(self) -> Dict[str, Any]:
        """××—×–×™×¨ ××ª ×›×œ ×”×”×’×“×¨×•×ª ×›××™×œ×•×Ÿ ×œ×©××™×¨×” ××• debug"""
        return {
            "search": self.search.__dict__,
            "embedding": self.embedding.__dict__,
            "chunk": self.chunk.__dict__,
            "context": {
                **self.context.__dict__,
                "available_context_tokens": self.context.AVAILABLE_CONTEXT_TOKENS
            },
            "llm": self.llm.__dict__,
            "database": self.database.__dict__,
            "performance": self.performance.__dict__,
            "optimization": self.optimization.__dict__
        }
    
    def validate_config(self) -> List[str]:
        """×‘×“×™×§×ª ×ª×§×™× ×•×ª ×”×”×’×“×¨×•×ª"""
        errors = []
        
        # ×‘×“×™×§×ª ×¡×¤×™ ×“××™×•×Ÿ
        if not 0.0 <= self.search.SIMILARITY_THRESHOLD <= 1.0:
            errors.append("SIMILARITY_THRESHOLD ×—×™×™×‘ ×œ×”×™×•×ª ×‘×™×Ÿ 0.0 ×œ-1.0")
        
        # ×‘×“×™×§×ª ×’×“×œ×™ ×¦'×× ×§
        if self.chunk.MIN_CHUNK_SIZE >= self.chunk.MAX_CHUNK_SIZE:
            errors.append("MIN_CHUNK_SIZE ×—×™×™×‘ ×œ×”×™×•×ª ×§×˜×Ÿ ×-MAX_CHUNK_SIZE")
        
        # ×‘×“×™×§×ª ×˜×•×§× ×™×
        if self.context.AVAILABLE_CONTEXT_TOKENS <= 0:
            errors.append("AVAILABLE_CONTEXT_TOKENS ×—×™×™×‘ ×œ×”×™×•×ª ×—×™×•×‘×™")
        
        # ×‘×“×™×§×ª ××©×§×œ×™× ×”×™×‘×¨×™×“×™×™×
        total_weight = self.search.HYBRID_SEMANTIC_WEIGHT + self.search.HYBRID_KEYWORD_WEIGHT
        if abs(total_weight - 1.0) > 0.01:
            errors.append("×¡×š ××©×§×œ×™ ×”×—×™×¤×•×© ×”×”×™×‘×¨×™×“×™ ×—×™×™×‘ ×œ×”×™×•×ª 1.0")
        
        return errors


# ×™×¦×™×¨×ª instance ×’×œ×•×‘×œ×™
rag_config = RAGConfig()

# ×‘×“×™×§×ª ×ª×§×™× ×•×ª ×‘×¢×ª ×™×™×‘×•×
config_errors = rag_config.validate_config()
if config_errors:
    print("âš ï¸ ×©×’×™××•×ª ×‘×”×’×“×¨×•×ª RAG:")
    for error in config_errors:
        print(f"  - {error}")
    print("×× × ×ª×§×Ÿ ××ª ×”×”×’×“×¨×•×ª ×‘×§×•×‘×¥ rag_config.py")


# ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×œ××¡××›×™ API
def get_search_config() -> SearchConfig:
    """××—×–×™×¨ ×”×’×“×¨×•×ª ×—×™×¤×•×©"""
    return rag_config.search

def get_embedding_config() -> EmbeddingConfig:
    """××—×–×™×¨ ×”×’×“×¨×•×ª embedding"""
    return rag_config.embedding

def get_chunk_config() -> ChunkConfig:
    """××—×–×™×¨ ×”×’×“×¨×•×ª ×¦'×× ×§×™×"""
    return rag_config.chunk

def get_context_config() -> ContextConfig:
    """××—×–×™×¨ ×”×’×“×¨×•×ª ×”×§×©×¨"""
    return rag_config.context

def get_llm_config() -> LLMConfig:
    """××—×–×™×¨ ×”×’×“×¨×•×ª LLM"""
    return rag_config.llm

def get_database_config() -> DatabaseConfig:
    """××—×–×™×¨ ×”×’×“×¨×•×ª ××¡×“ × ×ª×•× ×™×"""
    return rag_config.database

def get_performance_config() -> PerformanceConfig:
    """××—×–×™×¨ ×”×’×“×¨×•×ª ×‘×™×¦×•×¢×™×"""
    return rag_config.performance

def get_optimization_config() -> OptimizationConfig:
    """××—×–×™×¨ ×”×’×“×¨×•×ª ××•×¤×˜×™××™×–×¦×™×”"""
    return rag_config.optimization


if __name__ == "__main__":
    # ×‘×“×™×§×” ××”×™×¨×” ×©×œ ×”×”×’×“×¨×•×ª
    print("ğŸ”§ ×”×’×“×¨×•×ª ××¢×¨×›×ª RAG:")
    print("=" * 50)
    
    config_dict = rag_config.get_config_dict()
    for section, settings in config_dict.items():
        print(f"\nğŸ“‹ {section.upper()}:")
        for key, value in settings.items():
            print(f"  {key}: {value}")
    
    print(f"\nâœ… ×”×”×’×“×¨×•×ª ×ª×§×™× ×•×ª: {len(config_errors) == 0}") 