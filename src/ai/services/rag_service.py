import os
import asyncio
import logging
import time
import re
from typing import List, Dict, Any, Optional, Tuple
import json
import hashlib
from functools import lru_cache
from datetime import datetime, timedelta

import google.generativeai as genai
from supabase import create_client, Client
import numpy as np

try:
    # Try relative import first (when running from AI module)
    from ..config.current_profile import get_current_profile
    from ..config.rag_config_profiles import get_profile
    from ..config.rag_config import (
        rag_config,
        get_search_config,
        get_embedding_config,
        get_context_config,
        get_llm_config,
        get_database_config,
        get_performance_config
    )
    # Import vector utilities
    from ..utils.vector_utils import ensure_768_dimensions, log_vector_info
except ImportError:
    # Fallback to absolute import (when running from outside AI module)
    from src.ai.config.current_profile import get_current_profile
    from src.ai.config.rag_config_profiles import get_profile
    from src.ai.config.rag_config import (
        rag_config,
        get_search_config,
        get_embedding_config,
        get_context_config,
        get_llm_config,
        get_database_config,
        get_performance_config
    )
    # Import vector utilities
    from src.ai.utils.vector_utils import ensure_768_dimensions, log_vector_info

# ×‘×™×™×‘×•× ×”×—×“×©
try:
    # Try relative import first
    from ..core.database_key_manager import DatabaseKeyManager
except ImportError:
    # Fallback to absolute import
    from src.ai.core.database_key_manager import DatabaseKeyManager

logger = logging.getLogger(__name__)

class RAGService:
    # Class-level cache for embeddings (shared across instances)
    _embedding_cache = {}
    _cache_ttl = 300  # 5 minutes TTL
    
    def __init__(self, config_profile: Optional[str] = None):
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_SERVICE_KEY") or os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError("Missing SUPABASE_URL or SUPABASE_KEY environment variables")
        
        self.supabase: Client = create_client(supabase_url, supabase_key)
        
        # ğŸ†• ×”×—×œ×£ ×œ-Database Key Manager ×¢× ×—×™×‘×•×¨ ×™×©×™×¨ ×œ-Supabase
        self.key_manager = DatabaseKeyManager(use_direct_supabase=True)
        logger.info("ğŸ”‘ RAG Service using Database Key Manager with direct Supabase connection")
        
        # ğŸ”¥ FORCE AGGRESSIVE CONFIG - Skip profile system, use direct config
        logger.info("ğŸ”¥ Using AGGRESSIVE tuned config - bypassing profile system")
        self.search_config = get_search_config()
        self.embedding_config = get_embedding_config()
        self.context_config = get_context_config()
        self.llm_config = get_llm_config()
        self.db_config = get_database_config()
        self.performance_config = get_performance_config()
        
        # ğŸ”¥ ×”×’×“×¨ Key Manager - × ×˜×¢×Ÿ ××¤×ª×—×•×ª ×‘×¦×•×¨×” lazy
        # Note: Keys will be loaded when first needed in ensure_available_key()
        logger.info("ğŸ”‘ Database Key Manager configured - keys will be loaded on first use")
        
        # ğŸ”¥ ×™×¦×™×¨×ª ××•×“×œ Gemini - ×™×© fallback ×œ×¡×‘×™×‘×”
        # Try to get key from environment first (safe init approach)
        fallback_key = os.getenv("GEMINI_API_KEY")
        if fallback_key:
            genai.configure(api_key=fallback_key)
            logger.info("ğŸ”‘ Using GEMINI_API_KEY from environment for initialization")
        else:
            # Try to use Key Manager for initialization (keys loaded lazily) 
            try:
                if self.key_manager:
                    # Just configure with environment key for now, database keys will be loaded lazily
                    logger.info("ğŸ”‘ Database Key Manager configured - will use environment key for init")
                    # Will switch to database keys when needed
                else:
                    raise Exception("No API keys available from Database or environment")
            except Exception as e:
                logger.error(f"âŒ Key initialization failed: {e}")
                raise Exception("No API keys available")
        
        # ×™×¦×™×¨×ª ××•×“×œ Gemini ×¢× ×”×’×“×¨×•×ª ××”config
        self.model = genai.GenerativeModel(
            self.llm_config.MODEL_NAME,
            generation_config=genai.types.GenerationConfig(
                temperature=self.llm_config.TEMPERATURE,
                max_output_tokens=self.llm_config.MAX_OUTPUT_TOKENS
            )
        )
        
        logger.info(f"ğŸš€ RAG Service initialized with profile '{config_profile or 'default'}' - "
                   f"Similarity threshold: {self.search_config.SIMILARITY_THRESHOLD}, "
                   f"Max chunks: {self.search_config.MAX_CHUNKS_RETRIEVED}, "
                   f"Model: {self.llm_config.MODEL_NAME}")
    
    def _get_cache_key(self, text: str) -> str:
        """Generate cache key from text"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_entry: dict) -> bool:
        """Check if cache entry is still valid"""
        return (datetime.now() - cache_entry['timestamp']).seconds < self._cache_ttl
    
    def _get_from_cache(self, cache_key: str) -> Optional[List[float]]:
        """Get embedding from cache if valid"""
        if cache_key in self._embedding_cache:
            entry = self._embedding_cache[cache_key]
            if self._is_cache_valid(entry):
                logger.info(f"âš¡ Using cached embedding for key: {cache_key[:8]}")
                return entry['embedding']
            else:
                # Remove expired cache entry
                del self._embedding_cache[cache_key]
        return None
    
    def _cache_embedding(self, cache_key: str, embedding: List[float]):
        """Cache embedding with timestamp"""
        self._embedding_cache[cache_key] = {
            'embedding': embedding,
            'timestamp': datetime.now()
        }
        # Clean old cache entries (keep max 1000 entries)
        if len(self._embedding_cache) > 1000:
            # Remove oldest 100 entries
            sorted_items = sorted(self._embedding_cache.items(), key=lambda x: x[1]['timestamp'])
            for key, _ in sorted_items[:100]:
                del self._embedding_cache[key]

    async def _track_embedding_usage(self, text: str, key_id: Optional[int] = None):
        """Track token usage for embedding generation"""
        # Estimate tokens for embedding (much smaller than generation)
        estimated_tokens = len(text) // 8  # Embeddings use fewer tokens
        logger.info(f"ğŸ”¢ [RAG-EMBED-TRACK] Estimated {estimated_tokens} tokens for embedding")
        
        # ğŸ”¥ FIX: Actually track usage with key manager
        try:
            if self.key_manager and key_id:
                await self.key_manager.record_usage(key_id, estimated_tokens, 1)
                logger.info(f"ğŸ”¢ [RAG-EMBED-TRACK] Successfully tracked {estimated_tokens} tokens for key {key_id}")
            else:
                logger.warning("âš ï¸ [RAG-EMBED-TRACK] No key_id provided or key_manager not available")
        except Exception as e:
            logger.error(f"âŒ [RAG-EMBED-TRACK] Failed to track usage: {e}")

    async def _track_generation_usage(self, prompt: str, response: str, key_id: Optional[int] = None):
        """Track token usage for text generation"""
        input_tokens = len(prompt) // 4
        output_tokens = len(response) // 4
        total_tokens = input_tokens + output_tokens
        logger.info(f"ğŸ”¢ [RAG-GEN-TRACK] Estimated {total_tokens} tokens ({input_tokens} input + {output_tokens} output)")
        
        # ğŸ”¥ FIX: Actually track usage with key manager
        try:
            if self.key_manager and key_id:
                await self.key_manager.record_usage(key_id, total_tokens, 1)
                logger.info(f"ğŸ”¢ [RAG-GEN-TRACK] Successfully tracked {total_tokens} tokens for key {key_id}")
            else:
                logger.warning("âš ï¸ [RAG-GEN-TRACK] No key_id provided or key_manager not available")
        except Exception as e:
            logger.error(f"âŒ [RAG-GEN-TRACK] Failed to track usage: {e}")
    
    async def generate_query_embedding(self, query: str) -> List[float]:
        """×™×•×¦×¨ embedding ×¢×‘×•×¨ ×©××™×œ×ª×” ×¢× caching"""
        cache_key = self._get_cache_key(query)
        
        # âš¡ Check cache first
        cached_embedding = self._get_from_cache(cache_key)
        if cached_embedding:
            return cached_embedding
        
        try:
            # ğŸ”¥ Ensure we're using current key if Key Manager available
            key_id = None
            if self.key_manager:
                available_key = await self.key_manager.get_available_key()
                if not available_key:
                    raise Exception("No available API keys for embedding")
                
                # Configure with the available key
                genai.configure(api_key=available_key['api_key'])
                key_id = available_key.get('id')
                logger.info(f"ğŸ”‘ Using key {available_key.get('key_name', 'unknown')} for embedding")
            
            # â±ï¸ Generate embedding
            start_time = time.time()
            embedding_model = genai.embed_content(
                model=self.embedding_config.MODEL_NAME,
                content=query,
                task_type=self.embedding_config.TASK_TYPE_QUERY
            )
            
            raw_embedding = embedding_model['embedding']
            generation_time = int((time.time() - start_time) * 1000)
            
            # ×•×™×“×•× ×©×”-vector ×”×•× ×‘×“×™×•×§ 768 dimensions
            embedding = ensure_768_dimensions(raw_embedding)
            
            # ×¨×™×©×•× ××™×“×¢ ×œdebug ×× ×™×© ×‘×¢×™×” ×¢× ×’×•×“×œ ×”vector
            if len(raw_embedding) != 768:
                logger.warning(f"Query embedding dimension adjusted from {len(raw_embedding)} to 768")
                log_vector_info(raw_embedding, "Original query embedding")
                log_vector_info(embedding, "Adjusted query embedding")
            
            # âš¡ Cache the result
            self._cache_embedding(cache_key, embedding)
            
            # ğŸ”¥ Track usage
            await self._track_embedding_usage(query, key_id)
            
            logger.info(f"ğŸ” Embedding generated in {generation_time}ms (cached for future use, final dimensions: {len(embedding)})")
            return embedding
            
        except Exception as e:
            logger.error(f"Error generating query embedding: {e}")
            raise
    
    async def semantic_search(
        self, 
        query: str, 
        document_id: Optional[int] = None,
        max_results: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """×—×™×¤×•×© ×¡×× ×˜×™ ×‘××¡××›×™×"""
        start_time = time.time()
        
        try:
            # ×™×¦×™×¨×ª embedding ×¢×‘×•×¨ ×”×©××™×œ×ª×”
            query_embedding = await self.generate_query_embedding(query)
            
            # ×‘×™×¦×•×¢ ×—×™×¤×•×© ×‘×××¦×¢×•×ª RPC
            max_results = max_results or self.search_config.MAX_CHUNKS_RETRIEVED
            
            response = self.supabase.rpc(self.db_config.SEMANTIC_SEARCH_FUNCTION, {
                'query_embedding': query_embedding,
                'match_threshold': self.search_config.SIMILARITY_THRESHOLD,
                'match_count': max_results,
                'target_document_id': document_id
            }).execute()
            
            results = response.data or []
            response_time = int((time.time() - start_time) * 1000)
            
            # ×¨×™×©×•× analytics
            if self.performance_config.LOG_SEARCH_ANALYTICS:
                await self._log_search_analytics(
                    query, 
                    'semantic', 
                    len(results),
                    results[0]['similarity_score'] if results else 0.0,
                    response_time,
                    document_id=document_id
                )
            
            logger.info(f"Semantic search completed: {len(results)} results in {response_time}ms")
            return results
            
        except Exception as e:
            logger.error(f"Error in semantic search: {e}")
            raise
    
    async def hybrid_search(
        self, 
        query: str, 
        document_id: Optional[int] = None,
        semantic_weight: Optional[float] = None,
        keyword_weight: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """×—×™×¤×•×© ×”×™×‘×¨×™×“×™ (×¡×× ×˜×™ + ××™×œ×•×ª ××¤×ª×—)"""
        start_time = time.time()
        
        try:
            query_embedding = await self.generate_query_embedding(query)
            
            # ×©×™××•×© ×‘××©×§×œ×™× ××”config ×× ×œ× ×¡×•×¤×§×•
            semantic_weight = semantic_weight or self.search_config.HYBRID_SEMANTIC_WEIGHT
            keyword_weight = keyword_weight or self.search_config.HYBRID_KEYWORD_WEIGHT
            
            # Build parameters for the hybrid search function
            search_params = {
                'query_text': query,
                'query_embedding': query_embedding,
                'match_threshold': self.search_config.SIMILARITY_THRESHOLD,
                'match_count': self.search_config.MAX_CHUNKS_RETRIEVED,
                'semantic_weight': semantic_weight,
                'keyword_weight': keyword_weight
            }
            
            # Add document_id parameter only if provided (for backward compatibility)
            if document_id is not None:
                search_params['target_document_id'] = document_id
            
            response = self.supabase.rpc(self.db_config.HYBRID_SEARCH_FUNCTION, search_params).execute()
            
            results = response.data or []
            response_time = int((time.time() - start_time) * 1000)
            
            if self.performance_config.LOG_SEARCH_ANALYTICS:
                await self._log_search_analytics(
                    query, 
                    'hybrid', 
                    len(results),
                    results[0]['combined_score'] if results else 0.0,
                    response_time,
                    document_id=document_id
                )
            
            logger.info(f"Hybrid search completed: {len(results)} results in {response_time}ms")
            return results
            
        except Exception as e:
            logger.error(f"Error in hybrid search: {e}")
            raise
    
    async def contextual_search(
        self,
        query: str,
        section_filter: Optional[str] = None,
        content_type_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """×—×™×¤×•×© ××•×ª× ×” ×œ×¤×™ ×”×§×©×¨"""
        try:
            query_embedding = await self.generate_query_embedding(query)
            
            response = self.supabase.rpc(self.db_config.CONTEXTUAL_SEARCH_FUNCTION, {
                'query_embedding': query_embedding,
                'section_filter': section_filter,
                'content_type_filter': content_type_filter,
                'similarity_threshold': self.search_config.SIMILARITY_THRESHOLD,
                'max_results': self.search_config.MAX_RESULTS_EXTENDED
            }).execute()
            
            return response.data or []
            
        except Exception as e:
            logger.error(f"Error in contextual search: {e}")
            raise

    async def section_specific_search(
        self,
        query: str,
        target_section: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """×—×™×¤×•×© ××™×•×—×“ ×œ××¡×¤×¨×™ ×¡×¢×™×¤×™× ×¡×¤×¦×™×¤×™×™×"""
        try:
            start_time = time.time()
            
            # ×–×™×”×•×™ ××¡×¤×¨ ×¡×¢×™×£ ×‘×©××œ×”
            section_patterns = [
                r'×¡×¢×™×£\s+(\d+(?:\.\d+)*)',
                r'×‘×¡×¢×™×£\s+(\d+(?:\.\d+)*)',
                r'××”\s+(?:××•××¨|×›×ª×•×‘|× ×××¨)\s+(?:×‘)?×¡×¢×™×£\s+(\d+(?:\.\d+)*)',
                r'(\d+(?:\.\d+)+)(?:\s|$)',  # ××¡×¤×¨×™ ×¡×¢×™×¤×™× ×›××• 1.5.1
            ]
            
            extracted_section = target_section
            if not extracted_section:
                for pattern in section_patterns:
                    match = re.search(pattern, query)
                    if match:
                        extracted_section = match.group(1)
                        logger.info(f"Extracted section number: {extracted_section}")
                        break
            
            if extracted_section:
                # ×—×™×¤×•×© ××™×•×—×“ ×œ×¡×¢×™×¤×™× ×¢× pattern matching
                query_embedding = await self.generate_query_embedding(query)
                
                response = self.supabase.rpc('section_specific_search', {
                    'query_embedding': query_embedding,
                    'section_number': extracted_section,
                    'similarity_threshold': self.search_config.SECTION_SEARCH_THRESHOLD,
                    'max_results': self.search_config.MAX_CHUNKS_FOR_CONTEXT
                }).execute()
                
                section_results = response.data or []
                
                # ×× ×œ× × ××¦× ×“×‘×¨, ×—×¤×© ×’× ×—×™×¤×•×© ×”×™×‘×¨×™×“×™ ×¨×’×™×œ
                if not section_results:
                    logger.info(f"No section-specific results found for {extracted_section}, falling back to hybrid search")
                    section_results = await self.hybrid_search(query)
                
                response_time = int((time.time() - start_time) * 1000)
                logger.info(f"Section search completed: {len(section_results)} results in {response_time}ms")
                
                return section_results
            else:
                # ×× ×œ× × ××¦× ××¡×¤×¨ ×¡×¢×™×£, ×—×–×•×¨ ×œ×—×™×¤×•×© ×¨×’×™×œ
                logger.info("No section number detected, falling back to hybrid search")
                return await self.hybrid_search(query)
                
        except Exception as e:
            logger.error(f"Error in section specific search: {e}")
            # ×—×–×¨×” ×œ×—×™×¤×•×© ×¨×’×™×œ ×‘××§×¨×” ×©×œ ×©×’×™××”
            return await self.hybrid_search(query)

    def _build_context(self, search_results: List[Dict[str, Any]]) -> Tuple[str, List[str], List[Dict[str, Any]]]:
        """×‘×•× ×” ×§×•× ×˜×§×¡×˜ ××ª×•×¦××•×ª ×”×—×™×¤×•×© ×•××—×–×™×¨ ×’× ××ª ×”chunks ×©×‘×¤×•×¢×œ × ×›×œ×œ×•"""
        context_chunks = []
        citations = []
        included_chunks = []  # ğŸ†• ×¨×©×™××ª ×”×—×œ×§×™× ×©×‘×¤×•×¢×œ × ×›×œ×œ×• ×‘×§×•× ×˜×§×¡×˜
        total_tokens = 0
        
        # âœ… ×”×¡×¨×” ×©×œ hard-coding! ×¢×›×©×™×• × ×¡×ª××š ×¢×œ ××œ×’×•×¨×™×ª× ×—×›× ×‘×œ×‘×“
        # ×”×—×™×¤×•×© ×›×‘×¨ ××“×•×¨×’ ×œ×¤×™ ×¨×œ×•×•× ×˜×™×•×ª - × ×©××•×¨ ×¢×œ ×”×¡×“×¨ ×”×–×”
        reordered_results = search_results
        
        # ××’×‘×™×œ ×œ××¡×¤×¨ chunks ××§×¡×™××œ×™ ×•×œ×’×‘×•×œ tokens
        max_chunks_for_context = min(
            len(reordered_results), 
            self.search_config.MAX_CHUNKS_FOR_CONTEXT
        )
        
        for i, result in enumerate(reordered_results[:max_chunks_for_context]):
            chunk_content = result.get('chunk_text', result.get('content', ''))
            document_name = result.get('document_name', f'××¡××š {i+1}')
            
            # ×”×¢×¨×›×ª tokens (×‘×§×™×¨×•×‘)
            estimated_tokens = len(chunk_content.split()) * self.performance_config.TOKEN_ESTIMATION_MULTIPLIER
            
            if total_tokens + estimated_tokens > self.context_config.MAX_CONTEXT_TOKENS:
                logger.info(f"Context token limit reached at chunk {i}")
                break
            
            # ×”×•×¡×¤×ª ××™×“×¢ ×¢×œ ×”×¦×™×•×Ÿ ×“×•××™×•×ª ×× ×–××™×Ÿ
            similarity_info = ""
            if 'similarity_score' in result:
                similarity_info = f" (×“×•××™×•×ª: {result['similarity_score']:.3f})"
            elif 'combined_score' in result:
                similarity_info = f" (×¦×™×•×Ÿ: {result['combined_score']:.3f})"
            
            context_chunks.append(f"××§×•×¨ {len(included_chunks)+1} - {document_name}{similarity_info}:\n{chunk_content}")
            citations.append(document_name)
            included_chunks.append(result)  # ğŸ†• ×©××™×¨×ª ×”×—×œ×§ ×©× ×›×œ×œ
            total_tokens += estimated_tokens
        
        context = "\n\n".join(context_chunks)
        
        logger.info(f"Built context from {len(context_chunks)} chunks, ~{int(total_tokens)} tokens")
        return context, citations, included_chunks

    def _find_best_chunk_for_display(self, search_results: List[Dict[str, Any]], query: str) -> Optional[Dict[str, Any]]:
        """××•×¦× ××ª ×”×—×œ×§ ×”×›×™ ×¨×œ×•×•× ×˜×™ ×œ×”×¦×’×” ×‘-UI - ×—×™×¤×•×© ×—×›× ×¢× ×“×’×© ×¢×œ ×‘×™×˜×•×™×™× ××“×•×™×§×™×
        
        âš ï¸ DEPRECATED: ×¤×•× ×§×¦×™×” ×–×• ×œ× ×‘×©×™××•×© ×™×•×ª×¨. ×‘××§×•× ×–×”, ×”××•×“×œ ××¦×˜×˜ ××ª ×”××§×•×¨×•×ª ×©×”×•× ××©×ª××© ×‘×”×
        ×‘×××¦×¢×•×ª _extract_cited_sources ×•-_get_cited_chunks.
        """
        if not search_results:
            return None
        
        query_lower = query.lower()
        
        # ğŸ¯ ×§×•×“× ×›×œ - ×—×™×¤×•×© ×‘×™×˜×•×™×™× ××“×•×™×§×™× ××”×©××œ×”
        exact_phrases = []
        if '××Ÿ ×”×× ×™×™×Ÿ' in query_lower:
            exact_phrases.append('××Ÿ ×”×× ×™×™×Ÿ')
        if '×¢×œ ×ª× ××™' in query_lower:
            exact_phrases.append('×¢×œ ×ª× ××™')
        if '×•×¢×“×ª ××œ×’×•×ª' in query_lower:
            exact_phrases.append('×•×¢×“×ª ××œ×’×•×ª')
        
        # ×× ×™×© ×‘×™×˜×•×™ ××“×•×™×§, ×—×¤×© ××•×ª×• ×¨××©×•×Ÿ
        if exact_phrases:
            for chunk in search_results:
                chunk_text = chunk.get('chunk_text', chunk.get('content', ''))
                for phrase in exact_phrases:
                    if phrase in chunk_text:
                        logger.info(f"ğŸ¯ Found exact phrase '{phrase}' in chunk - selecting it")
                        return chunk
        
        # ××™×œ×•×ª ××¤×ª×— ×œ× ×•×©××™× ×©×•× ×™×
        topic_keywords = {
            'parking': ['×—× ×™', '×—× ×”', '×§× ×¡', '××’×¨×©', '×¨×›×‘'],
            'scholarships': ['××œ×’×”', '××œ×’×•×ª', '×•×¢×“×ª', '×¡×™×•×¢', '×‘×§×©×”'],
            'grades': ['×¦×™×•×Ÿ', '×‘×—×™× ×”', '××‘×—×Ÿ', '×”×¢×¨×›×”'],
            'tuition': ['×©×›×¨', '×œ×™××•×“', '×ª×©×œ×•×', '×›×¡×£'],
            'discipline': ['××©××¢×ª', '×¢×‘×™×¨×”', '×¢×•× ×©'],
            'student_status': ['××Ÿ ×”×× ×™×™×Ÿ', '×¢×œ ×ª× ××™', '×¡×˜×•×“× ×˜', '××¢××“']
        }
        
        # ×–×™×”×•×™ × ×•×©× ×”×©××œ×”
        query_topic = None
        for topic, keywords in topic_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                query_topic = topic
                break
        
        best_chunk = None
        best_score = 0
        
        for chunk in search_results:
            chunk_text = chunk.get('chunk_text', chunk.get('content', ''))
            chunk_lower = chunk_text.lower()
            
            score = 0
            
            # ×‘×•× ×•×¡ ×’×‘×•×” ×××•×“ ×œ×‘×™×˜×•×™×™× ××“×•×™×§×™×
            for phrase in exact_phrases:
                if phrase in chunk_text:
                    score += self.search_config.EXACT_PHRASE_BONUS
            
            # ×× ×–×™×”×™× ×• × ×•×©×, ×—×¤×© ××™×œ×•×ª ××¤×ª×— ×¨×œ×•×•× ×˜×™×•×ª
            if query_topic and query_topic in topic_keywords:
                relevant_keywords = topic_keywords[query_topic]
                topic_matches = sum(1 for keyword in relevant_keywords if keyword in chunk_lower)
                score += topic_matches * self.search_config.TOPIC_MATCH_BONUS
            
            # ×‘×•× ×•×¡ ×œ××™×œ×•×ª ××¤×ª×— ××”×©××œ×” ×¢×¦××”
            query_words = [word.strip() for word in query.split() if len(word.strip()) > 2]
            direct_matches = sum(1 for word in query_words if word in chunk_text)
            score += direct_matches * self.search_config.DIRECT_MATCH_BONUS
            
            # ×‘×•× ×•×¡ ×œ×“×•××™×•×ª ×’×‘×•×”×” (××©×§×œ × ××•×š ×™×•×ª×¨)
            similarity = chunk.get('similarity_score', chunk.get('similarity', 0))
            score += similarity * self.search_config.SIMILARITY_WEIGHT_FACTOR
            
            if score > best_score:
                best_score = score
                best_chunk = chunk
        
        # ×× ×œ× × ××¦× match ×˜×•×‘, ×§×— ××ª ×–×” ×¢× ×”×“×•××™×•×ª ×”×’×‘×•×”×” ×‘×™×•×ª×¨
        if best_chunk is None and search_results:
            best_chunk = max(search_results, 
                           key=lambda x: x.get('similarity_score', x.get('similarity', 0)))
        
        logger.info(f"ğŸ¯ Selected chunk with score: {best_score}")
        return best_chunk

    def _create_rag_prompt(self, query: str, context: str) -> str:
        """×™×•×¦×¨ prompt ××•×ª×× ×œ×©××œ×•×ª ×ª×§× ×•× ×™× ×¢× ×”× ×—×™×” ×œ×¦×™×˜×•×˜ ××§×•×¨×•×ª"""
        return f"""××ª×” ×¢×•×–×¨ ××§×“××™ ×”××ª××—×” ×‘×ª×§× ×•× ×™ ××›×œ×œ×ª ××¤×§×”. ×¢× ×” ×¢×œ ×”×©××œ×” ×‘×”×ª×‘×¡×¡ ×¢×œ ×”××™×“×¢ ×”×¨×œ×•×•× ×˜×™ ×©× ×™×ª×Ÿ.

×”×§×©×¨ ×¨×œ×•×•× ×˜×™ ××”×ª×§× ×•× ×™×:
{context}

×©××œ×ª ×”××©×ª××©: {query}

âš ï¸ CRITICAL: ×¦×™×˜×•×˜ ××§×•×¨×•×ª ×”×•× ×—×•×‘×” ××‘×¡×•×œ×•×˜×™×ª ×•×‘×œ×ª×™ ××©×ª××¢×ª! ×œ×œ× ×—×¨×™×’×™×!

×”× ×—×™×•×ª ×œ××ª×Ÿ ×ª×©×•×‘×”:
1. ×§×¨× ×‘×§×¤×™×“×” ××ª ×›×œ ×”××™×“×¢ ×©× ×™×ª×Ÿ ××”×§×©×¨ ×œ×¢×™×œ
2. ×¢× ×” ×‘×¢×‘×¨×™×ª ×‘×¦×•×¨×” ×‘×¨×•×¨×” ×•××¤×•×¨×˜×ª, ×›×•×œ×œ ×¤×¨×˜×™× ×¡×¤×¦×™×¤×™×™× ×›××• ×¡×›×•××™×, ××—×•×–×™×, ×ª× ××™×
3. ×× ×”××™×“×¢ ×§×™×™× ×‘×§×©×¨ - ×ª×Ÿ ×ª×©×•×‘×” ××œ××” ×•××“×•×™×§×ª
4. ×× ×”×©××œ×” × ×•×’×¢×ª ×œ×¡×¢×™×£ ×¡×¤×¦×™×¤×™, ×¦×˜×˜ ××•×ª×• ×‘××“×•×™×§
5. ×× ×”××™×“×¢ ×—×œ×§×™ ××• ×œ× ×‘×¨×•×¨, ×¦×™×™×Ÿ ×–××ª ×•×ª×Ÿ ××ª ×”××™×“×¢ ×©×›×Ÿ ×§×™×™×
6. ×× ×”×©××œ×” ×œ× ×§×©×•×¨×” ×œ×ª×§× ×•× ×™× ×›×œ×œ, ×¦×™×™×Ÿ ×©××™×Ÿ ×œ×š ××™×“×¢ ×¢×œ ×”× ×•×©×
7. ×‘××§×¨×” ×©×œ ××œ×’×•×ª, ×–×›×•×™×•×ª ××• ×”×˜×‘×•×ª - ×¤×¨×˜ ××ª ×›×œ ×”×ª× ××™× ×•×”×¡×›×•××™× ×”×¨×œ×•×•× ×˜×™×™×

ğŸ”¥ MANDATORY SOURCE CITATION FORMAT:
×‘×¡×•×£ ×›×œ ×ª×©×•×‘×”, ×—×™×™×‘ ×œ×›×œ×•×œ ×¦×™×˜×•×˜ ××§×•×¨×•×ª ×‘×¤×•×¨××˜ ×”×–×” ×‘×“×™×•×§:
[××§×•×¨×•×ª: ××§×•×¨ X, ××§×•×¨ Y]

â­ REQUIRED EXAMPLES:
âœ… CORRECT: "×”×§× ×¡ ×¢×œ ×—× ×™×” ×”×•× 150 â‚ª. [××§×•×¨×•×ª: ××§×•×¨ 1, ××§×•×¨ 3]"
âœ… CORRECT: "×–××Ÿ ×”×œ×™××•×“×™× ×œ×ª×•××¨ ×‘×”× ×“×¡×” ×”×•× 4 ×©× ×™×. [××§×•×¨×•×ª: ××§×•×¨ 2]"
âœ… CORRECT: "×ª× ××™ ×”×–×›××•×ª ×œ××œ×’×” ×›×•×œ×œ×™×... [××§×•×¨×•×ª: ××§×•×¨ 5, ××§×•×¨ 12]"

âŒ WRONG: ×ª×©×•×‘×” ×œ×œ× ×¦×™×˜×•×˜ = ×ª×©×•×‘×” ×œ× ×ª×§×™× ×”!
âŒ WRONG: "××§×•×¨: ××¡××š X" - ×¤×•×¨××˜ ×©×’×•×™!
âŒ WRONG: "[××§×•×¨ 1]" - ×—×¡×¨ ×”××™×œ×” "××§×•×¨×•×ª"!

ğŸ¯ STEP-BY-STEP INSTRUCTION:
1. ×›×ª×‘ ××ª ×”×ª×©×•×‘×” ×”××œ××”
2. ×–×”×” ××™×–×” ××§×•×¨×•×ª (××§×•×¨ 1, ××§×•×¨ 2, ×•×›×•') ×”×©×ª××©×ª ×‘×”×
3. ×”×•×¡×£ ×‘×¡×•×£: [××§×•×¨×•×ª: ××§×•×¨ X, ××§×•×¨ Y]
4. ×‘×“×•×§ ×©×”×¤×•×¨××˜ ××“×•×™×§!

âš ï¸ ×–×›×•×¨: ×”××¢×¨×›×ª ×ª×“×—×” ×›×œ ×ª×©×•×‘×” ×œ×œ× ×¦×™×˜×•×˜ ××§×•×¨×•×ª ×‘××“×•×™×§ ×”×¤×•×¨××˜ ×”× ×“×¨×©!

×ª×©×•×‘×”:"""

    def _extract_cited_sources(self, answer: str) -> List[int]:
        """××—×œ×¥ ××ª ×”××§×•×¨×•×ª ×©×¦×™×˜×˜ ×”××•×“×œ ××”×ª×©×•×‘×” ×¢× validation ×—×–×§"""
        import re
        
        # ×¤×˜×¨× ×™× ×—×œ×•×¤×™×™× ×œ×–×™×”×•×™ ×¦×™×˜×•×˜×™×
        patterns = [
            r'\[××§×•×¨×•×ª:\s*([^\]]+)\]',  # ×”×¤×˜×¨×Ÿ ×”×¡×˜× ×“×¨×˜×™
            r'\[××§×•×¨:\s*([^\]]+)\]',    # ×œ×œ× ×¡' ×‘×¨×‘×™×
            r'××§×•×¨×•×ª:\s*([^\n\r]+)',   # ×œ×œ× ×¡×•×’×¨×™×™× ××¨×•×‘×¢×™×
            r'××§×•×¨:\s*([^\n\r]+)',     # ×œ×œ× ×¡' ×‘×¨×‘×™× ×•×œ×œ× ×¡×•×’×¨×™×™×
            r'\(××§×•×¨×•×ª:\s*([^\)]+)\)', # ×¢× ×¡×•×’×¨×™×™× ×¢×’×•×œ×™×
        ]
        
        sources_text = None
        pattern_used = None
        
        for i, pattern in enumerate(patterns):
            match = re.search(pattern, answer, re.IGNORECASE)
            if match:
                sources_text = match.group(1)
                pattern_used = i + 1
                logger.info(f"ğŸ” ××§×•×¨×•×ª × ××¦××• ×¢× ×¤×˜×¨×Ÿ #{pattern_used}: {sources_text}")
                break
        
        if not sources_text:
            # ××–×”×¨×” ×—××•×¨×” - Gemini ×œ× ×¦×™×˜×˜ ××§×•×¨×•×ª
            logger.error("ğŸš¨ CRITICAL: ×œ× × ××¦××• ××§×•×¨×•×ª ××¦×•×˜×˜×™× ×‘×ª×©×•×‘×”! Gemini ×œ× ×¦×™×™×ª ×œ×”×•×¨××•×ª!")
            logger.error(f"ğŸ“ ×ª×©×•×‘×” ××œ××”: {answer}")
            return []
        
        # ×—×™×œ×•×¥ ××¡×¤×¨×™ ×”××§×•×¨×•×ª ×¢× validation ××ª×§×“×
        source_numbers = []
        source_pattern = r'××§×•×¨\s*(\d+)'
        source_matches = re.findall(source_pattern, sources_text, re.IGNORECASE)
        
        for match in source_matches:
            try:
                source_num = int(match)
                if 1 <= source_num <= 100:  # validation ×¡×‘×™×¨ ×œ××¡×¤×¨ ××§×•×¨×•×ª
                    source_numbers.append(source_num)
                else:
                    logger.warning(f"âš ï¸ ××¡×¤×¨ ××§×•×¨ ×œ× ×¡×‘×™×¨: {source_num}")
            except ValueError:
                logger.warning(f"âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×”××™×¨ ×œ××¡×¤×¨: {match}")
                continue
        
        if not source_numbers:
            logger.error(f"ğŸš¨ ×œ× × ××¦××• ××¡×¤×¨×™ ××§×•×¨×•×ª ×ª×§×™× ×™× ×‘×˜×§×¡×˜: {sources_text}")
        else:
            logger.info(f"âœ… ××§×•×¨×•×ª ××¦×•×˜×˜×™× ×‘×”×¦×œ×—×”: {source_numbers}")
        
        return source_numbers

    async def _get_cited_chunks(self, included_chunks: List[Dict[str, Any]], cited_source_numbers: List[int], query: str = "", answer: str = "") -> List[Dict[str, Any]]:
        """××—×–×™×¨ ××ª ×”chunks ×©×‘×××ª ×¦×•×˜×˜×• ×¢×œ ×™×“×™ ×”××•×“×œ ××ª×•×š ×”chunks ×©× ×›×œ×œ×• ×‘×§×•× ×˜×§×¡×˜"""
        if not cited_source_numbers:
            # ×× ×œ× × ××¦××• ×¦×™×˜×•×˜×™×, ××¦× ××ª ×”chunk ×”×›×™ ×¨×œ×•×•× ×˜×™ ×‘×××¦×¢×•×ª semantic similarity
            logger.warning("âš ï¸ ×œ× × ××¦××• ×¦×™×˜×•×˜×™× ××”××•×“×œ - ××—×¤×© chunk ×¨×œ×•×•× ×˜×™ ×œ×ª×©×•×‘×” ×‘×××¦×¢×•×ª ×“××™×•×Ÿ ×¡×× ×˜×™")
            return await self._find_best_fallback_chunk_semantic(included_chunks, answer)
        
        # ××¡×•×£ ××ª ×›×œ ×”chunks ×©×¦×•×˜×˜×•
        cited_chunks = []
        for source_num in cited_source_numbers:
            # ×”××§×•×¨×•×ª ××ª×—×™×œ×™× ×-1, ××‘×œ ×”××™× ×“×§×¡ ××ª×—×™×œ ×-0
            index = source_num - 1
            if 0 <= index < len(included_chunks):
                cited_chunks.append(included_chunks[index])
                logger.info(f"âœ… ××¦× ××§×•×¨ ××¦×•×˜×˜ {source_num} (××ª×•×š {len(included_chunks)} chunks ×‘×§×•× ×˜×§×¡×˜)")
            else:
                logger.warning(f"âš ï¸ ××§×•×¨ {source_num} ×œ× ×§×™×™× ×‘×§×•× ×˜×§×¡×˜ (×™×© ×¨×§ {len(included_chunks)} ××§×•×¨×•×ª)")
        
        if not cited_chunks:
            logger.warning("âš ï¸ ×œ× × ××¦××• chunks ×ª×§×™× ×™× ××”×¦×™×˜×•×˜×™× - ×¢×•×‘×¨ ×œ-semantic fallback")
            return await self._find_best_fallback_chunk_semantic(included_chunks, answer)
        
        # ğŸ¯ ×× ×™×© ×™×•×ª×¨ ×××§×•×¨ ××—×“ ×©×¦×•×˜×˜, ×‘×—×¨ ××ª ×”×›×™ ×¨×œ×•×•× ×˜×™ ×œ×ª×©×•×‘×”
        if len(cited_chunks) > 1:
            logger.info(f"ğŸ” × ××¦××• {len(cited_chunks)} ××§×•×¨×•×ª ××¦×•×˜×˜×™× - ×‘×•×—×¨ ××ª ×”×›×™ ×¨×œ×•×•× ×˜×™ ×œ×ª×©×•×‘×”")
            best_chunk = await self._find_best_among_cited_chunks(cited_chunks, answer)
            return [best_chunk]
        
        return cited_chunks

    async def _find_best_among_cited_chunks(self, cited_chunks: List[Dict[str, Any]], answer: str) -> Dict[str, Any]:
        """×‘×•×—×¨ ××ª ×”chunk ×”×›×™ ×¨×œ×•×•× ×˜×™ ××‘×™×Ÿ ×”chunks ×©×¦×•×˜×˜×• ×¢×œ ×™×“×™ ×”LLM"""
        if not cited_chunks:
            raise ValueError("Cannot find best chunk from empty list")
        
        if not answer:
            logger.warning("âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×‘×—×•×¨ ××‘×™×Ÿ cited chunks - ××—×–×™×¨ ×¨××©×•×Ÿ")
            return cited_chunks[0]
        
        if len(cited_chunks) == 1:
            return cited_chunks[0]
            
        try:
            # ×™×¦×™×¨×ª embedding ×œ×ª×©×•×‘×”
            answer_embedding = await self.generate_query_embedding(answer)
            
            # ×—×™×©×•×‘ similarity ×¢×‘×•×¨ ×›×œ chunk ××¦×•×˜×˜ + × ×™×ª×•×— ×ª×•×›×Ÿ
            scored_chunks = []
            
            for chunk in cited_chunks:
                chunk_text = chunk.get('chunk_text', chunk.get('content', ''))
                chunk_id = chunk.get('id', 'unknown')
                
                if chunk_text:
                    # ×™×¦×™×¨×ª embedding ×œchunk
                    chunk_embedding = await self.generate_query_embedding(chunk_text)
                    
                    # ×—×™×©×•×‘ cosine similarity
                    similarity = np.dot(answer_embedding, chunk_embedding) / (
                        np.linalg.norm(answer_embedding) * np.linalg.norm(chunk_embedding)
                    )
                    
                    # × ×™×ª×•×— ×ª×•×›×Ÿ ×œ×”×’×‘×¨×ª ×“×™×•×§
                    content_score = 0
                    chunk_lower = chunk_text.lower()
                    answer_lower = answer.lower()
                    
                    # ×‘×•× ×•×¡ ×œ××™×œ×™× ××©×•×ª×¤×•×ª
                    answer_words = set(answer_lower.split())
                    chunk_words = set(chunk_lower.split())
                    common_words = answer_words.intersection(chunk_words)
                    content_score += len(common_words) * 0.01
                    
                    # ×‘×•× ×•×¡ ×œ××™×œ×•×ª ××¤×ª×— ×—×©×•×‘×•×ª
                    key_phrases = {
                        '××Ÿ ×”×× ×™×™×Ÿ': 0.15,
                        '×¢×œ ×ª× ××™': 0.1,
                        '×¢×•××“ ×‘×ª× ××™': 0.12,
                        '×“×¨×™×©×•×ª': 0.08,
                        '×§×‘×œ×”': 0.08,
                        '×ª×•×›× ×™×ª ×œ×™××•×“×™×': 0.1
                    }
                    
                    for phrase, weight in key_phrases.items():
                        if phrase in chunk_lower and phrase in answer_lower:
                            content_score += weight
                    
                    # ×¦×™×•×Ÿ ××©×•×œ×‘
                    combined_score = similarity + content_score
                    
                    logger.info(f"ğŸ” Cited chunk {chunk_id} - similarity: {similarity:.3f}, content: {content_score:.3f}, combined: {combined_score:.3f}")
                    
                    scored_chunks.append((chunk, combined_score, similarity))
            
            if scored_chunks:
                # ×‘×—×™×¨×ª ×”×˜×•×‘ ×‘×™×•×ª×¨ ×œ×¤×™ ×¦×™×•×Ÿ ××©×•×œ×‘
                best_chunk, best_combined, best_similarity = max(scored_chunks, key=lambda x: x[1])
                chunk_id = best_chunk.get('id', 'unknown')
                logger.info(f"ğŸ¯ × ×‘×—×¨ cited chunk {chunk_id} ×¢× ×¦×™×•×Ÿ ××©×•×œ×‘ {best_combined:.3f} (similarity: {best_similarity:.3f})")
                return best_chunk
            else:
                logger.warning("âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×—×©×‘ scores - ××—×–×™×¨ chunk ×¨××©×•×Ÿ")
                return cited_chunks[0]
                
        except Exception as e:
            logger.error(f"âŒ ×©×’×™××” ×‘×‘×—×™×¨×ª cited chunk: {e}")
            return cited_chunks[0]

    def _extract_relevant_chunk_segment(self, chunk_text: str, query: str, answer: str, max_length: int = 500) -> str:
        """××—×œ×¥ ××ª ×”×§×˜×¢ ×”×›×™ ×¨×œ×•×•× ×˜×™ ××”chunk ×œ×”×¦×’×” ×‘×××©×§"""
        if not chunk_text or len(chunk_text) <= max_length:
            return chunk_text
        
        try:
            # ×¤×™×¦×•×œ ×”×˜×§×¡×˜ ×œ××©×¤×˜×™×
            sentences = [s.strip() for s in re.split(r'[.!?]\s+', chunk_text) if s.strip()]
            if not sentences:
                return chunk_text[:max_length] + "..."
            
            # ××™×œ×•×ª ××¤×ª×— ××”×©××œ×” ×•×”×ª×©×•×‘×”
            query_words = set(re.findall(r'\b\w+\b', query.lower()))
            answer_words = set(re.findall(r'\b\w+\b', answer.lower()))
            key_words = query_words.union(answer_words)
            
            # ××™×œ×•×ª ××¤×ª×— ×—×©×•×‘×•×ª ×œ×ª×—×•×
            domain_keywords = {
                '××Ÿ ×”×× ×™×™×Ÿ', '×¢×œ ×ª× ××™', '×¢×•××“ ×‘×ª× ××™', '×“×¨×™×©×•×ª', '×§×‘×œ×”', 
                '×ª×•×›× ×™×ª ×œ×™××•×“×™×', '×¦×™×•×Ÿ', '×××•×¦×¢', '×¡×˜×•×“× ×˜', '×—× ×™×”', '××œ×’×”',
                '×–××Ÿ', '×©× ×™×', '×ª×§×•×¤×”', '××©×š', '×œ×™××•×“×™×'
            }
            
            # ×¦×™×•×Ÿ ×œ×›×œ ××©×¤×˜
            scored_sentences = []
            for i, sentence in enumerate(sentences):
                sentence_lower = sentence.lower()
                score = 0
                
                # ×‘×•× ×•×¡ ×œ××™×œ×™× ××”×©××œ×”/×ª×©×•×‘×”
                word_matches = sum(1 for word in key_words if word in sentence_lower and len(word) > 2)
                score += word_matches * 2
                
                # ×‘×•× ×•×¡ ×œ××™×œ×•×ª ××¤×ª×— ×©×œ ×”×ª×—×•×
                domain_matches = sum(1 for keyword in domain_keywords if keyword in sentence_lower)
                score += domain_matches * 3
                
                # ×‘×•× ×•×¡ ×œ××™×§×•× (××©×¤×˜×™× ×¨××©×•× ×™× ×™×•×ª×¨ ×—×©×•×‘×™×)
                position_bonus = max(0, self.search_config.POSITION_BONUS_BASE - i * self.search_config.POSITION_BONUS_DECAY)
                score += position_bonus
                
                # ×‘×•× ×•×¡ ×œ××•×¨×š ××ª××™× (×œ× ×§×¦×¨ ××“×™, ×œ× ××¨×•×š ××“×™)
                length = len(sentence)
                if 50 <= length <= 200:
                    score += 1
                elif length < 20:
                    score -= 2
                
                scored_sentences.append((sentence, score, i))
                logger.debug(f"ğŸ“ ××©×¤×˜ {i}: score={score:.2f}, length={length}")
            
            # ××™×•×Ÿ ×œ×¤×™ ×¦×™×•×Ÿ
            scored_sentences.sort(key=lambda x: x[1], reverse=True)
            
            # ×‘× ×™×™×ª ×”×§×˜×¢ ×”×¨×œ×•×•× ×˜×™
            selected_sentences = []
            current_length = 0
            used_indices = set()
            
            # ×”×•×¡×¤×ª ×”××©×¤×˜×™× ×”×˜×•×‘×™× ×‘×™×•×ª×¨ ×¢×“ ×œ××•×¨×š ×”××§×¡×™××œ×™
            for sentence, score, index in scored_sentences:
                if current_length + len(sentence) <= max_length:
                    selected_sentences.append((sentence, index))
                    used_indices.add(index)
                    current_length += len(sentence) + 1  # +1 for space
                    logger.debug(f"âœ… × ×‘×—×¨ ××©×¤×˜ {index} ×¢× ×¦×™×•×Ÿ {score:.2f}")
                
                if current_length >= max_length * self.performance_config.CONTEXT_TRIM_THRESHOLD:  # ××œ× ××¢×ª×” ×©×œ ×”×§×•× ×˜×§×¡×˜
                    break
            
            if not selected_sentences:
                # ×‘××§×¨×” ×©×œ× × ××¦× ×›×œ×•×, ×§×— ××ª ×”××©×¤×˜ ×”×¨××©×•×Ÿ
                return sentences[0][:max_length] + ("..." if len(sentences[0]) > max_length else "")
            
            # ××™×•×Ÿ ×œ×¤×™ ×¡×“×¨ ×”××§×•×¨×™ ×‘×˜×§×¡×˜
            selected_sentences.sort(key=lambda x: x[1])
            
            # ×‘× ×™×™×ª ×”×˜×§×¡×˜ ×”×¡×•×¤×™
            result = " ".join([sentence for sentence, _ in selected_sentences])
            
            # ×•×™×“×•× ×©×”×˜×§×¡×˜ ×œ× ×—×ª×•×š ×‘×××¦×¢ ××™×œ×”
            if len(result) >= max_length:
                result = result[:max_length].rsplit(' ', 1)[0] + "..."
            
            logger.info(f"ğŸ¯ ×”×•×¤×§ ×§×˜×¢ ×¨×œ×•×•× ×˜×™ ×‘××•×¨×š {len(result)} ×ª×•×•×™× ××ª×•×š {len(chunk_text)} ×”××§×•×¨×™×™×")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ×©×’×™××” ×‘×—×™×œ×•×¥ ×§×˜×¢ ×¨×œ×•×•× ×˜×™: {e}")
            # ×‘××§×¨×” ×©×œ ×©×’×™××”, ×—×–×•×¨ ×œ×˜×§×¡×˜ ×”×§×¦×•×¨ ×”×¤×©×•×˜
            return chunk_text[:max_length] + ("..." if len(chunk_text) > max_length else "")

    def _find_best_fallback_chunk(self, included_chunks: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """××•×¦× ××ª ×”chunk ×”×›×™ ×¨×œ×•×•× ×˜×™ ×›××©×¨ ××™×Ÿ ×¦×™×˜×•×˜×™× ××¤×•×¨×©×™×"""
        if not included_chunks:
            return []
        
        query_lower = query.lower()
        
        # ××™×œ×•×ª ××¤×ª×— ×œ×–×™×”×•×™ × ×•×©××™×
        topic_keywords = {
            'time_limits': ['×–××Ÿ', '×©× ×™×', '×©× ×”', '××§×¡×™××œ×™', '××©×š', '×ª×§×•×¤×”', '×œ×™××•×“×™×', '×¡×™×•×', '×”× ×“×¡×”', '××“×¢×™×', '×ª×•×›× ×™×ª', '×™×•×', '×¢×¨×‘', '×©× ×ª×™×™× ××¢×‘×¨', '×× ×™×™×Ÿ ×©× ×•×ª'],
            'parking': ['×—× ×™', '×—× ×”', '×§× ×¡', '×¨×›×‘', '××’×¨×©'],
            'scholarships': ['××œ×’×”', '××œ×’×•×ª', '×•×¢×“×”', '×•×¢×“×ª'],
            'grades': ['×¦×™×•×Ÿ', '×¦×™×•× ×™×', '×××•×¦×¢', '×’××¨'],
            'student_status': ['××Ÿ ×”×× ×™×™×Ÿ', '×¢×œ ×ª× ××™', '×¡×˜×•×“× ×˜'],
            'fees': ['×ª×©×œ×•×', '×©×›×¨ ×œ×™××•×“', '×“××™', '×¢×œ×•×ª']
        }
        
        # âœ… ×”×¡×¨×” ×©×œ hard-coding! ××œ×’×•×¨×™×ª× ×—×›× ×‘×œ×‘×“
        # ××™×Ÿ ×¢×•×“ hard-coded chunk IDs
        
        # ×—×¤×© chunks ×¢× ××™×œ×•×ª ××¤×ª×— ×¨×œ×•×•× ×˜×™×•×ª
        scored_chunks = []
        for chunk in included_chunks:
            content = chunk.get('chunk_text', chunk.get('content', '')).lower()
            score = 0
            chunk_id = chunk.get('id')
            
            # âœ… ×”×¡×¨×” ×©×œ hard-coding! ××™×Ÿ ×¢×•×“ ×—×©×™×‘×•×ª ×œ×¤×™ ID
            
            # ×¦×™×•×Ÿ ×‘×¡×™×¡×™ ×œ×¤×™ similarity
            base_score = chunk.get('similarity_score', chunk.get('combined_score', 0))
            score += base_score * 100
            
            # ×‘×•× ×•×¡ ×¢×‘×•×¨ ××™×œ×•×ª ××¤×ª×— ×¨×œ×•×•× ×˜×™×•×ª
            for topic, keywords in topic_keywords.items():
                topic_matches = sum(1 for keyword in keywords if keyword in query_lower)
                if topic_matches > 0:
                    content_matches = sum(1 for keyword in keywords if keyword in content)
                    bonus = content_matches * topic_matches * 20
                    
                    # ×‘×•× ×•×¡ ××™×•×—×“ ×œ×—×œ×§×™× ×¢× ××™×“×¢ ××“×•×™×§ ×¢×œ ×”× ×•×©×
                    if topic == 'time_limits' and any(phrase in content for phrase in ['×× ×™×™×Ÿ ×©× ×•×ª', '×©× ×ª×™×™× ××¢×‘×¨', '×ª×•×›× ×™×ª ×‘×ª', '×–××Ÿ ××•×ª×¨', '××©×š ××§×¡×™××œ×™']):
                        bonus *= 3  # ×¤×™ 3 ×œ×¦'×× ×§×™× ×¢× ××™×“×¢ ××“×•×™×§ ×¢×œ ×–××Ÿ
                        logger.info(f"ğŸ¯ × ××¦× ×¦'×× ×§ ××™×•×—×“ ×œ×–××Ÿ ×œ×™××•×“×™× ×¢× ×‘×™×˜×•×™×™× ×¨×œ×•×•× ×˜×™×™×")
                    
                    score += bonus
            
            # ×‘×•× ×•×¡ × ×•×¡×£ ×œ××™×œ×™× ××”×©××œ×” ×©××•×¤×™×¢×•×ª ×‘×ª×•×›×Ÿ
            query_words = [word for word in query_lower.split() if len(word) > 2]
            word_matches = sum(1 for word in query_words if word in content)
            score += word_matches * 10
            
            scored_chunks.append((chunk, score))
        
        # ××™×™×Ÿ ×œ×¤×™ ×¦×™×•×Ÿ ×•×‘×—×¨ ××ª ×”×˜×•×‘ ×‘×™×•×ª×¨
        scored_chunks.sort(key=lambda x: x[1], reverse=True)
        best_chunk = scored_chunks[0][0]
        best_score = scored_chunks[0][1]
        
        logger.info(f"ğŸ¯ × ×‘×—×¨ chunk fallback ×¢× ×¦×™×•×Ÿ {best_score:.1f}")
        
        return [best_chunk]

    async def _find_best_fallback_chunk_semantic(self, included_chunks: List[Dict[str, Any]], answer: str) -> List[Dict[str, Any]]:
        """××•×¦× ××ª ×”chunk ×”×›×™ ×“×•××” ×œ×ª×©×•×‘×” ×©× ×•×¦×¨×” ×‘×××¦×¢×•×ª cosine similarity"""
        if not included_chunks or not answer:
            logger.warning("âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×‘×¦×¢ semantic similarity - ××™×Ÿ chunks ××• ×ª×©×•×‘×”")
            return included_chunks[:1] if included_chunks else []
        
        try:
            # ×™×¦×™×¨×ª embedding ×œ×ª×©×•×‘×” ×”×¡×•×¤×™×ª
            logger.info("ğŸ§  ×™×•×¦×¨ embedding ×œ×ª×©×•×‘×” ×”×¡×•×¤×™×ª ×œ×¦×•×¨×š ×”×©×•×•××” ×¡×× ×˜×™×ª")
            answer_embedding = await self.generate_query_embedding(answer)
            
            # ×—×™×©×•×‘ cosine similarity ×¢×‘×•×¨ ×›×œ chunk
            similarities = []
            for chunk in included_chunks:
                chunk_text = chunk.get('chunk_text', chunk.get('content', ''))
                if chunk_text:
                    # ×™×¦×™×¨×ª embedding ×œchunk
                    chunk_embedding = await self.generate_query_embedding(chunk_text)
                    
                    # ×—×™×©×•×‘ cosine similarity
                    similarity = np.dot(answer_embedding, chunk_embedding) / (
                        np.linalg.norm(answer_embedding) * np.linalg.norm(chunk_embedding)
                    )
                    similarities.append((chunk, similarity))
                    logger.info(f"ğŸ” Chunk {chunk.get('id', 'unknown')} semantic similarity: {similarity:.3f}")
            
            if not similarities:
                logger.warning("âš ï¸ ×œ× × ×™×ª×Ÿ ×œ×—×©×‘ similarities - ××—×–×™×¨ chunk ×¨××©×•×Ÿ")
                return included_chunks[:1]
            
            # ×‘×—×™×¨×ª ×”chunk ×¢× ×”similarity ×”×’×‘×•×” ×‘×™×•×ª×¨
            best_chunk, best_similarity = max(similarities, key=lambda x: x[1])
            logger.info(f"ğŸ¯ × ×‘×—×¨ chunk {best_chunk.get('id', 'unknown')} ×¢× semantic similarity {best_similarity:.3f}")
            
            return [best_chunk]
            
        except Exception as e:
            logger.error(f"âŒ ×©×’×™××” ×‘-semantic similarity fallback: {e}")
            # ×‘××§×¨×” ×©×œ ×©×’×™××”, ×—×–×•×¨ ×œ×©×™×˜×” ×”×§×•×“××ª
            return included_chunks[:1] if included_chunks else []

    async def generate_answer(
        self, 
        query: str, 
        search_method: str = 'hybrid',
        document_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """×™×•×¦×¨ ×ª×©×•×‘×” ××œ××” ×œ×©××™×œ×ª×”"""
        start_time = time.time()
        
        try:
            logger.info(f"Generating answer for query: {query[:100]}...")
            
            # ×–×™×”×•×™ ×× ×–×• ×©××œ×” ×¢×œ ×¡×¢×™×£ ×¡×¤×¦×™×¤×™
            section_keywords = ['×¡×¢×™×£', '×‘×¡×¢×™×£', '×¤×¨×§', '×ª×§× ×”']
            is_section_query = any(keyword in query for keyword in section_keywords)
            
            # ×‘×™×¦×•×¢ ×—×™×¤×•×© ×œ×¤×™ ×”×©×™×˜×” ×”××‘×•×§×©×ª
            if is_section_query:
                logger.info("Detected section-specific query, using enhanced search")
                search_results = await self.section_specific_search(query)
            elif search_method == 'semantic':
                search_results = await self.semantic_search(query, document_id)
            elif search_method == 'hybrid':
                search_results = await self.hybrid_search(query, document_id)
            elif search_method == 'contextual':
                search_results = await self.contextual_search(query)
            else:
                raise ValueError(f"Unknown search method: {search_method}")
            
            if not search_results:
                return {
                    "answer": "×œ× × ××¦× ××™×“×¢ ×¨×œ×•×•× ×˜×™ ×‘×ª×§× ×•× ×™× ×œ×©××œ×” ×–×•. ×× × × ×¡×” ×œ× ×¡×— ××ª ×”×©××œ×” ×‘×¦×•×¨×” ××—×¨×ª ××• ×¤× ×” ×œ××©×¨×“ ×”×¡×˜×•×“× ×˜×™×.",
                    "sources": [],
                    "chunks_selected": [],
                    "search_results_count": 0,
                    "response_time_ms": int((time.time() - start_time) * 1000),
                    "search_method": search_method,
                    "query": query
                }
            
            # ×‘× ×™×™×ª ×”×§×©×¨
            context, citations, included_chunks = self._build_context(search_results)
            
            # ×™×¦×™×¨×ª prompt
            prompt = self._create_rag_prompt(query, context)
            
            # ğŸ” Debug: log the chunks being used
            logger.info(f"ğŸ” [CHUNKS-DEBUG] Using {len(search_results)} total chunks, {len(included_chunks)} included in context")
            for i, chunk in enumerate(included_chunks[:5]):  # Log first 5 chunks that were included
                similarity = chunk.get('similarity_score') or chunk.get('similarity', 0)
                chunk_preview = chunk.get('chunk_text', chunk.get('content', ''))[:100]
                logger.info(f"ğŸ” [CONTEXT-CHUNK-{i+1}] Similarity: {similarity:.3f} | Preview: {chunk_preview}")
            
            # ×©×™××•×© ×‘××¢×¨×›×ª ×¦×™×˜×•×˜ ××§×•×¨×•×ª ×—×“×©×” ×‘××§×•× ××œ×’×•×¨×™×ª× ×‘×—×™×¨×ª chunks ××•×¨×›×‘
            
            # ×™×¦×™×¨×ª ×ª×©×•×‘×” ×¢× retry logic
            answer = await self._generate_with_retry(prompt)
            
            # ğŸ¯ ×—×™×œ×•×¥ ×”××§×•×¨×•×ª ×©×”××•×“×œ ×‘×¤×•×¢×œ ×”×©×ª××© ×‘×”×
            cited_source_numbers = self._extract_cited_sources(answer)
            cited_chunks = await self._get_cited_chunks(included_chunks, cited_source_numbers, query, answer)
            
            # ×”×¡×¨×ª ×¦×™×˜×•×˜ ×”××§×•×¨×•×ª ××”×ª×©×•×‘×” ×”×¡×•×¤×™×ª (××•×¤×¦×™×•× ×œ×™)
            import re
            clean_answer = re.sub(r'\[××§×•×¨×•×ª:[^\]]+\]', '', answer).strip()
            
            # ğŸ¯ ×”×•×¡×¤×ª ×§×˜×¢ ×¨×œ×•×•× ×˜×™ ×œ×›×œ chunk ×©× ×‘×—×¨
            for chunk in cited_chunks:
                chunk_text = chunk.get('chunk_text', chunk.get('content', ''))
                if chunk_text:
                    relevant_segment = self._extract_relevant_chunk_segment(
                        chunk_text, query, clean_answer, max_length=500
                    )
                    chunk['relevant_segment'] = relevant_segment
                    logger.info(f"ğŸ¯ ×”×•×¡×¤×ª ×§×˜×¢ ×¨×œ×•×•× ×˜×™ ×œchunk {chunk.get('id', 'unknown')}: {len(relevant_segment)} ×ª×•×•×™×")
            
            response_time = int((time.time() - start_time) * 1000)
            
            result = {
                "answer": clean_answer,
                "sources": citations,
                "chunks_selected": cited_chunks,
                "search_results_count": len(search_results),
                "response_time_ms": response_time,
                "search_method": search_method,
                "query": query,
                "cited_sources": cited_source_numbers,  # ××™×“×¢ × ×•×¡×£ ×¢×œ ×”××§×•×¨×•×ª ×©×¦×•×˜×˜×•
                "config_used": {
                    "similarity_threshold": self.search_config.SIMILARITY_THRESHOLD,
                    "max_chunks": self.search_config.MAX_CHUNKS_RETRIEVED,
                    "model": self.llm_config.MODEL_NAME,
                    "temperature": self.llm_config.TEMPERATURE
                }
            }
            
            # ×‘×“×™×§×ª ×‘×™×¦×•×¢×™×
            if response_time > self.performance_config.MAX_GENERATION_TIME_MS:
                logger.warning(f"Generation time {response_time}ms exceeds target "
                             f"{self.performance_config.MAX_GENERATION_TIME_MS}ms")
            
            logger.info(f"Answer generated successfully in {response_time}ms with {len(search_results)} chunks")
            return result
            
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            raise

    async def _generate_with_retry(self, prompt: str, max_retries: int = 3) -> str:
        """×™×•×¦×¨ ×ª×•×›×Ÿ ×¢× retry logic ×œ××§×¨×” ×©×œ rate limiting"""
        for attempt in range(max_retries):
            try:
                # ğŸ”¥ Ensure we're using current key if Key Manager available
                if self.key_manager:
                    available_key = await self.key_manager.get_available_key()
                    if not available_key:
                        raise Exception("No available API keys for generation")
                    
                    # Configure with the available key
                    genai.configure(api_key=available_key['api_key'])
                    
                    # Recreate model with current key
                    self.model = genai.GenerativeModel(
                        self.llm_config.MODEL_NAME,
                        generation_config=genai.types.GenerationConfig(
                            temperature=self.llm_config.TEMPERATURE,
                            max_output_tokens=self.llm_config.MAX_OUTPUT_TOKENS
                        )
                    )
                
                logger.info(f"ğŸ”¢ [RAG-GEN-DEBUG] Generating response for prompt length: {len(prompt)}")
                
                # Debug: Log first part of prompt to check content
                logger.info(f"ğŸ” [PROMPT-DEBUG] First 500 chars: {prompt[:500]}")
                logger.info(f"ğŸ” [PROMPT-DEBUG] Last 200 chars: {prompt[-200:]}")
                
                response = await self.model.generate_content_async(prompt)
                response_text = response.text
                
                logger.info(f"ğŸ” [RESPONSE-DEBUG] Raw response: {response_text[:200]}")
                
                # ğŸ”¥ Track usage  
                key_id = available_key.get('id') if available_key else None
                await self._track_generation_usage(prompt, response_text, key_id)
                
                logger.info(f"ğŸ”¢ [RAG-GEN-DEBUG] Generated response length: {len(response_text)}")
                return response_text
                
            except Exception as e:
                error_str = str(e)
                
                # ×‘×“×™×§×” ×× ×–×” rate limiting error
                if "429" in error_str or "quota" in error_str.lower():
                    if attempt < max_retries - 1:
                        wait_time = (2 ** attempt) * self.performance_config.RETRY_BACKOFF_BASE  # exponential backoff
                        logger.warning(f"Rate limit hit, waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}")
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        logger.error(f"Max retries reached for rate limiting")
                        return "××¦×˜×¢×¨, ×”××¢×¨×›×ª ×¢××•×¡×” ×›×¨×’×¢. ×× × × ×¡×” ×©×•×‘ ×‘×¢×•×“ ×›××” ×“×§×•×ª."
                else:
                    # ×©×’×™××” ××—×¨×ª - ×”×¢×œ×” ××™×“
                    raise
        
        # ×× ×”×’×¢× ×• ×œ×›××Ÿ, ×—×¨×’× ×• ××›×œ ×”× ×™×¡×™×•× ×•×ª
        logger.error("Exhausted all retry attempts")
        return "××¦×˜×¢×¨, ×”××¢×¨×›×ª × ×ª×§×œ×” ×‘×‘×¢×™×” ×˜×›× ×™×ª. ×× × × ×¡×” ×©×•×‘ ×××•×—×¨ ×™×•×ª×¨."
    
    async def _log_search_analytics(
        self, 
        query: str, 
        search_type: str, 
        results_count: int,
        top_score: float, 
        response_time_ms: int,
        document_id: Optional[int] = None
    ):
        """×¨×•×©× × ×ª×•× ×™ ×—×™×¤×•×© ×œ×˜×‘×œ×ª analytics"""
        try:
            analytics_data = {
                "query_text": query,
                "search_type": search_type,
                "results_count": results_count,
                "top_score": top_score,
                "response_time_ms": response_time_ms,
                "config_profile": get_current_profile() if 'get_current_profile' in locals() else 'default',
            }

            # The document_id is currently an integer and causes a UUID error in the RPC.
            # Temporarily removing it from the log until the DB schema is fixed.
            # if document_id is not None:
            #     analytics_data["document_id"] = document_id

            response = self.supabase.rpc(
                self.db_config.LOG_ANALYTICS_FUNCTION, 
                analytics_data
            ).execute()
            
            # Check for errors in the response
            if hasattr(response, 'error') and response.error:
                logger.warning(f"Failed to log search analytics: {response.error.message}")
            elif isinstance(response, dict) and response.get('error'):
                 logger.warning(f"Failed to log search analytics: {response.get('error')}")

        except Exception as e:
            # Handle cases where `response` might not be a standard object
            if "postgrest.exceptions.APIError" in str(type(e)):
                 logger.warning(f"Failed to log search analytics: {e.message}")
            else:
                 logger.warning(f"Failed to log search analytics: {e}")
    
    async def get_search_statistics(self, days_back: int = 30) -> Dict[str, Any]:
        """××—×–×™×¨ ×¡×˜×˜×™×¡×˜×™×§×•×ª ×—×™×¤×•×©"""
        try:
            # ×‘×“×™×§×” ×× analytics ××•×¤×¢×œ
            if not self.performance_config.LOG_SEARCH_ANALYTICS:
                return {"error": "Analytics disabled in configuration"}
                
            response = self.supabase.rpc('get_search_statistics', {
                'days_back': days_back
            }).execute()
            
            return response.data[0] if response.data else {}
            
        except Exception as e:
            logger.error(f"Error getting search statistics: {e}")
            return {"error": str(e)}
    
    def get_current_config(self) -> Dict[str, Any]:
        """××—×–×™×¨ ××ª ×”×”×’×“×¨×•×ª ×”× ×•×›×—×™×•×ª"""
        return rag_config.get_config_dict()


# --- Dependency Injection (if using Flask-Injector or similar) ---
# You might use a framework for dependency injection, or a simple factory
_rag_service_instance = None

def get_rag_service() -> RAGService:
    """Provides a singleton instance of the RAGService."""
    global _rag_service_instance
    if _rag_service_instance is None:
        _rag_service_instance = RAGService()
    return _rag_service_instance
