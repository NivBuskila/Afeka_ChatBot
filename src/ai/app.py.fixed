from flask import Flask, request, jsonify, Response
import os
import logging
import time
from functools import wraps
import google.generativeai as genai
import sys
from pathlib import Path
import dotenv
from flask_cors import CORS  # הוספת ייבוא של flask_cors
import asyncio

# הוספת הנתיב לתיקיית backend כדי לאפשר גישה למודולים של RAG
backend_path = Path(__file__).parent.parent / "backend"
sys.path.insert(0, str(backend_path))

# טעינת משתני סביבה
dotenv.load_dotenv(override=True)

# ייבוא מודולי RAG
try:
    from services.document_processor import DocumentProcessor
    from app.core.database import get_supabase_client
    from app.core.rag_service import RAGService
    has_rag = True
    # Initialize document processor once
    doc_processor = DocumentProcessor()
    # Initialize RAG service with new parameters
    rag_service = RAGService(
        doc_processor=doc_processor,
        enable_reranking=os.getenv('RAG_ENABLE_RERANKING', 'True').lower() == 'true',
        default_search_limit=int(os.getenv('RAG_DEFAULT_SEARCH_LIMIT', 6)),
        default_search_threshold=float(os.getenv('RAG_DEFAULT_SEARCH_THRESHOLD', 0.72)),
        cross_encoder_model_name=os.getenv('RAG_CROSS_ENCODER_MODEL', 'cross-encoder/ms-marco-MiniLM-L-6-v2'),
        llm_model_name=os.getenv('RAG_LLM_MODEL', 'gemini-1.5-flash-latest')
    )
    logger.info("RAGService initialized successfully.")
except ImportError as e:
    logger.error(f"RAG modules could not be imported: {e}. RAG features will be disabled.")
    has_rag = False
    rag_service = None
    doc_processor = None # If RAGService init depends on it, and RAGService fails, ensure it's clear
except Exception as e_init:
    logger.error(f"Error initializing RAGService: {e_init}. RAG features will be disabled.")
    logger.error(traceback.format_exc())
    has_rag = False
    rag_service = None
    doc_processor = None # Ensure doc_processor is also None if it was part of the failed init sequence

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize Gemini API
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
if not GEMINI_API_KEY:
    logger.warning("GEMINI_API_KEY not found in environment variables, using default")
    GEMINI_API_KEY = 'AIzaSyBBw-VlqWekqnd_vPXCS7LSuKfrkbOro7s'
# שימוש ב-configure במקום ביצירת מופע Client
genai.configure(api_key=GEMINI_API_KEY)

# Create Flask app
app = Flask(__name__)
# פתרון בעיית CORS - הגדרה פשוטה
CORS(app, supports_credentials=True)

# Get environment settings
DEBUG = os.environ.get('DEBUG', 'False').lower() == 'true'
API_RATE_LIMIT = int(os.environ.get('API_RATE_LIMIT', '60'))  # Requests per minute per IP
MAX_MESSAGE_LENGTH = int(os.environ.get('MAX_MESSAGE_LENGTH', '2000'))  # Maximum length of input message

# Add security headers and CORS headers to all responses
@app.after_request
def add_headers(response):
    # Security headers
    response.headers['X-Content-Type-Options'] = 'nosniff'
    response.headers['X-Frame-Options'] = 'DENY'
    response.headers['X-XSS-Protection'] = '1; mode=block'
    
    # Explicitly add CORS headers
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, DELETE, OPTIONS'
    response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'
    
    return response

# Basic route for health checks
@app.route('/')
def health_check():
    """Basic health check endpoint"""
    return jsonify({
        "status": "ok", 
        "service": "ai-service",
        "rag_support": has_rag
    })

# נקודת קצה לטיפול בבקשות OPTIONS עבור CORS
@app.route('/chat', methods=['OPTIONS'])
def handle_options():
    """Handle OPTIONS pre-flight requests for CORS"""
    response = jsonify({'status': 'ok'})
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Methods'] = 'POST, OPTIONS'
    response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
    response.headers['Access-Control-Max-Age'] = '86400'  # 24 hours
    return response

# RAG endpoints
@app.route('/rag/search', methods=['POST'])
def rag_search():
    """חיפוש סמנטי במסמכים"""
    if not has_rag or not doc_processor: # Check doc_processor too
        return jsonify({"error": "RAG modules not available"}), 503
    
    try:
        data = request.get_json(force=True)
        query = data.get("query", "")
        limit = data.get("limit", 10)
        threshold = data.get("threshold", 0.78)
        
        if not query.strip():
            return jsonify({"error": "Query cannot be empty"}), 400
        
        # Use asyncio.run() to execute the async function
        results = asyncio.run(doc_processor.search_documents(query, limit, threshold))
        
        return jsonify({
            "query": query,
            "results": results,
            "count": len(results)
        })
        
    except Exception as e:
        logger.error(f"Error in semantic search: {str(e)}")
        logger.error(traceback.format_exc()) # Add traceback
        return jsonify({"error": str(e), "traceback": traceback.format_exc()}), 500

@app.route('/rag/search/hybrid', methods=['POST'])
def rag_hybrid_search():
    """חיפוש היברידי (סמנטי + מילות מפתח)"""
    if not has_rag or not doc_processor: # Check doc_processor too
        return jsonify({"error": "RAG modules not available"}), 503
    
    try:
        data = request.get_json(force=True)
        query = data.get("query", "")
        limit = data.get("limit", 10)
        threshold = data.get("threshold", 0.78)
        
        if not query.strip():
            return jsonify({"error": "Query cannot be empty"}), 400
        
        # Use asyncio.run() to execute the async function
        results = asyncio.run(doc_processor.hybrid_search(query, limit, threshold))
        
        return jsonify({
            "query": query,
            "results": results,
            "count": len(results)
        })
        
    except Exception as e:
        logger.error(f"Error in hybrid search: {str(e)}")
        logger.error(traceback.format_exc()) # Add traceback
        return jsonify({"error": str(e), "traceback": traceback.format_exc()}), 500

@app.route('/rag/stats', methods=['GET'])
def rag_stats():
    """מידע על מסמכים ו-embeddings"""
    if not has_rag or doc_processor is None:
        return jsonify({"error": "RAG components (doc_processor) not available"}), 503
    
    try:
        # Database stats from DocumentProcessor
        db_stats = asyncio.run(doc_processor.get_stats())
        
        # RAGService operational stats
        service_stats = {}
        if rag_service:
            service_stats = rag_service.get_service_stats()
        else:
            service_stats = {"message": "RAGService not initialized or available."}
            
        # RAGService configuration (runtime)
        service_config = {}
        if rag_service:
            service_config = rag_service.get_runtime_config()
        else:
            service_config = {"message": "RAGService not initialized or available."}

        combined_stats = {
            "database_stats": db_stats,
            "rag_service_operational_stats": service_stats,
            "rag_service_runtime_config": service_config
        }
        return jsonify(combined_stats)
    except Exception as e:
        logger.error(f"Error fetching RAG stats: {e}")
        logger.error(traceback.format_exc())
        return jsonify({"error": "Failed to retrieve RAG statistics", "details": str(e)}), 500

@app.route('/rag/document/<int:document_id>', methods=['GET'])
def rag_document_status(document_id):
    """מידע מפורט על תהליך עיבוד המסמך"""
    if not has_rag:
        return jsonify({"error": "RAG modules not available"}), 503
    
    try:
        supabase = get_supabase_client()
        
        # Get document details
        doc_result = supabase.table("documents").select("*").eq("id", document_id).execute()
        if not doc_result.data:
            return jsonify({"error": f"Document {document_id} not found"}), 404
            
        document = doc_result.data[0]
        
        # Get chunks count
        chunks_result = supabase.table("document_chunks").select("*", count="exact").eq("document_id", document_id).execute()
        chunks_count = chunks_result.count if hasattr(chunks_result, 'count') else 0
        
        # Calculate progress percentage
        progress = 0
        if document['processing_status'] == 'pending':
            progress = 0
        elif document['processing_status'] == 'processing':
            if chunks_count > 0:
                # If we have chunks but still processing, estimate progress
                progress = min(95, int(chunks_count / 0.5))  # Estimate 50 chunks for full processing
            else:
                progress = 20  # Initial processing stage
        elif document['processing_status'] == 'completed':
            progress = 100
        elif document['processing_status'] == 'failed':
            progress = 0
        
        # Get processing time if available
        processing_time = None
        if document.get('updated_at') and document.get('created_at'):
            from datetime import datetime
            created = datetime.fromisoformat(document['created_at'].replace('Z', '+00:00'))
            updated = datetime.fromisoformat(document['updated_at'].replace('Z', '+00:00'))
            processing_time = (updated - created).total_seconds()
        
        return jsonify({
            "document": {
                "id": document['id'],
                "name": document['name'],
                "status": document['processing_status'],
                "created_at": document['created_at'],
                "updated_at": document['updated_at'],
                "embedding_model": document.get('embedding_model')
            },
            "chunks_count": chunks_count,
            "progress": progress,
            "processing_time": processing_time
        })
        
    except Exception as e:
        logger.error(f"Error getting document status: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/rag/test', methods=['POST'])
def rag_test():
    """
    נקודת קצה לבדיקת שירות ה-RAG החדש
    """
    if not has_rag or rag_service is None:
        return jsonify({"error": "RAG service not available or not initialized properly"}), 503
    
    try:
        data = request.get_json(force=True)
        query = data.get("query", "")
        if not query.strip():
            return jsonify({"error": "Query cannot be empty"}), 400

        use_hybrid = data.get("use_hybrid", False)
        add_sources = data.get("add_sources", True)
        limit = data.get("limit") 
        threshold = data.get("threshold")
        
        if data.get("search_only", False):
            search_results = asyncio.run(rag_service.search(query, limit=limit, threshold=threshold, use_hybrid=use_hybrid))
            return jsonify({"query": query, "search_results": search_results, "count": len(search_results)})
        
        if data.get("rerank_only", False):
            search_results = asyncio.run(rag_service.search(query, limit=limit, threshold=threshold, use_hybrid=use_hybrid))
            reranked_results = asyncio.run(rag_service.rerank_results(query, search_results))
            return jsonify({"query": query, "reranked_results": reranked_results, "count": len(reranked_results)})

        if data.get("context_only", False):
            search_results = asyncio.run(rag_service.search(query, limit=limit, threshold=threshold, use_hybrid=use_hybrid))
            reranked_results = asyncio.run(rag_service.rerank_results(query, search_results))
            context_str = rag_service._build_context_string(reranked_results, 
                                                            max_tokens_for_context=data.get("context_max_tokens", 2500),
                                                            max_chunks_for_context=data.get("context_max_chunks",3))
            return jsonify({"query": query, "context": context_str, "reranked_results_count": len(reranked_results)})
        
        response_data = asyncio.run(rag_service.get_answer(
            query,
            use_hybrid_search=use_hybrid,
            add_sources_to_response=add_sources,
            search_limit_override=limit,
            search_threshold_override=threshold,
            context_max_tokens=data.get("context_max_tokens", 2500),
            context_max_chunks=data.get("context_max_chunks", 3)
        ))
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error in RAG test endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e), "traceback": traceback.format_exc()}), 500

@app.route('/rag/debug', methods=['POST'])
def rag_debug():
    """דיבאג מפורט של שירות ה-RAG"""
    if not has_rag or rag_service is None:
        return jsonify({"error": "RAG service not available or not initialized properly"}), 503
    
    try:
        data = request.get_json(force=True)
        query = data.get("query", "")
        if not query.strip():
            return jsonify({"error": "Query cannot be empty"}), 400
        
        use_hybrid = data.get("use_hybrid", False)
        limit = data.get("limit")
        threshold = data.get("threshold")

        raw_search_results = asyncio.run(rag_service.search(query, limit=limit, threshold=threshold, use_hybrid=use_hybrid))
        reranked_results = asyncio.run(rag_service.rerank_results(query, raw_search_results))
        
        # Accessing __defaults__ can be brittle. Get defaults from RAGService config or pass explicitly.
        # For simplicity, using direct values or request data for now.
        context_max_tokens = data.get("context_max_tokens", rag_service.get_runtime_config().get("context_max_tokens", 2500)) # Example if config held this
        context_max_chunks = data.get("context_max_chunks", rag_service.get_runtime_config().get("context_max_chunks", 3)) # Example if config held this
        # Fallback to direct defaults if not in config from RAGService
        context_max_tokens = data.get("context_max_tokens", 2500) 
        context_max_chunks = data.get("context_max_chunks", 3)

        context_str = rag_service._build_context_string(reranked_results, max_tokens=context_max_tokens, max_chunks=context_max_chunks)
        
        llm_prompt_for_debug = f"Context:\n{context_str}\n\nUser: {query}"
        if not context_str: llm_prompt_for_debug = f"User: {query}"
        
        full_response_for_debug = asyncio.run(rag_service.get_answer(query, use_hybrid_search=use_hybrid, add_sources_to_response=False, search_limit_override=limit, search_threshold_override=threshold, context_max_tokens=context_max_tokens, context_max_chunks=context_max_chunks))
        ai_answer_preview = full_response_for_debug.get("result", "Error getting preview")
        
        debug_info = {
            "query": query,
            "params": {"use_hybrid": use_hybrid, "limit": limit, "threshold": threshold, "context_max_tokens": context_max_tokens, "context_max_chunks": context_max_chunks},
            "raw_search_results_count": len(raw_search_results),
            "raw_search_results_preview": [{k: v for k, v in r.items() if k != 'embedding'} for r in raw_search_results[:3]],
            "reranked_results_count": len(reranked_results),
            "reranked_results_preview": [{k: v for k, v in r.items() if k != 'embedding'} for r in reranked_results[:3]],
            "context_preview": context_str[:1000] + ("..." if len(context_str) > 1000 else ""),
            "context_length_chars": len(context_str),
            "llm_prompt_preview_for_debug": llm_prompt_for_debug[:1000] + ("..." if len(llm_prompt_for_debug) > 1000 else ""),
            "final_answer_from_service_preview": ai_answer_preview[:400] + ("..." if len(ai_answer_preview) > 400 else ""),
            "rag_service_config": rag_service.get_runtime_config(),
            "rag_service_stats_current_session": rag_service.get_service_stats() # These are cumulative for the service instance
        }
        return jsonify(debug_info)
        
    except Exception as e:
        logger.error(f"Error in RAG debug endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e), "traceback": traceback.format_exc()}), 500

# Main chat endpoint
@app.route('/chat', methods=['POST'])
def chat():
    """
    Process user message with Gemini API
    """
    request_start_time = time.time()
    
    try:
        # Log raw request for debugging
        logger.info(f"Request content type: {request.content_type}")
        
        # Get raw data and decode manually if needed
        try:
            # Try parsing as JSON first
            data = request.get_json(force=True, silent=True)
            if not data and request.data:
                # If JSON parsing failed, try manual decoding
                raw_data = request.data.decode('utf-8', errors='replace')
                logger.info(f"Raw data (length {len(raw_data)}): {raw_data[:100]}...")
                import json
                data = json.loads(raw_data)
        except Exception as json_err:
            logger.error(f"JSON parsing error: {str(json_err)}")
            # If JSON parsing failed completely, check raw data
            data = {}
            if request.data:
                logger.info(f"Raw request data (bytes): {request.data[:100]}")
        
        if not data or 'message' not in data:
            logger.warning("Invalid request: missing message field")
            response = jsonify({
                "error": "Message field is required"
            })
            response.headers['Content-Type'] = 'application/json; charset=utf-8'
            return response, 400
            
        user_message = data['message']
        logger.info(f"User message type: {type(user_message)}")
        
        # Ensure message is properly decoded if it's bytes
        if isinstance(user_message, bytes):
            user_message = user_message.decode('utf-8', errors='replace')
        
        # Validate message length
        if not user_message or len(user_message) > MAX_MESSAGE_LENGTH:
            logger.warning(f"Message too long: {len(user_message)} chars (max: {MAX_MESSAGE_LENGTH})")
            response = jsonify({
                "error": f"Message too long (max {MAX_MESSAGE_LENGTH} characters)"
            })
            response.headers['Content-Type'] = 'application/json; charset=utf-8'
            return response, 400
                
        logger.info(f"Received message (length {len(user_message)}): {user_message[:30]}...")
        
        # בדיקה אם להשתמש ב-RAG
        use_rag_flag = has_rag and 'use_rag' in data and data['use_rag'] and rag_service is not None
        
        try:
            # הגדרת מודל Gemini ישירות כברירת מחדל
            model = genai.GenerativeModel("gemini-2.0-flash")
            
            if use_rag_flag:
                # שימוש במודול ה-RAG החדש
                add_sources = data.get('add_sources', True)
                use_hybrid = data.get('use_hybrid', False)
                s_limit = data.get('rag_search_limit')
                s_thresh = data.get('rag_search_threshold')
                ctx_tokens = data.get('rag_context_max_tokens', 2500)
                ctx_chunks = data.get('rag_context_max_chunks', 3)
                
                # Use asyncio.run() for the async call
                response_data = asyncio.run(rag_service.get_answer(
                    user_message, 
                    use_hybrid_search=use_hybrid, 
                    add_sources_to_response=add_sources,
                    search_limit_override=s_limit,
                    search_threshold_override=s_thresh,
                    context_max_tokens=ctx_tokens,
                    context_max_chunks=ctx_chunks
                ))
                ai_response = response_data.get('result', '')
                logger.info(f"RAG enhanced response received: {ai_response[:30]}...")
                
                # הכנת התשובה עם כל המידע שהתקבל מה-RAG
                result = {
                    "result": ai_response,
                    "processing_time": response_data.get('processing_time_s', round(time.time() - request_start_time, 3)),
                    "rag_used": response_data.get('rag_used', False),
                    "rag_count": response_data.get('rag_count', 0),
                    "sources": response_data.get('sources', [])
                }
            else:
                # שימוש ישיר במודל ללא RAG
                gemini_response = model.generate_content(user_message)
                ai_response = gemini_response.text
                logger.info(f"Standard Gemini response received: {ai_response[:30]}...")
                
                # הכנת תשובה סטנדרטית
                result = {
                    "result": ai_response,
                    "processing_time": round(time.time() - request_start_time, 3),
                    "rag_used": False,
                    "rag_count": 0
                }
        except Exception as api_error:
            logger.error(f"API error: {str(api_error)}")
            ai_response = "מצטער, אירעה שגיאה בעת עיבוד הבקשה שלך. אנא נסה שוב מאוחר יותר."
        
            # הכנת תשובת שגיאה
            result = {
                "result": ai_response,
                "processing_time": round(time.time() - request_start_time, 3),
                "rag_used": False,
                "rag_count": 0,
                "error": str(api_error)
            }
        
        logger.info(f"Message processed successfully in {result['processing_time']}s")
        response = jsonify(result)
        response.headers['Content-Type'] = 'application/json; charset=utf-8'
        return response
        
    except Exception as e:
        import traceback
        logger.error(f"Unexpected error in chat endpoint: {str(e)}")
        logger.error(traceback.format_exc())
        response = jsonify({
            "error": "Internal server error",
            "message": str(e)
        })
        response.headers['Content-Type'] = 'application/json; charset=utf-8'
        return response, 500

if __name__ == '__main__':
    # Set up server start time for uptime tracking
    app.start_time = time.time()
    
    # Get port from environment variable or use default
    port = int(os.environ.get('PORT', 5000))
    
    # Run the Flask app
    logger.info(f"Starting AI service on port {port}")
    logger.info(f"RAG support: {'Enabled' if has_rag else 'Disabled'}")
    app.run(
        host='0.0.0.0', 
        port=port, 
        debug=DEBUG,
        threaded=True
    ) 
