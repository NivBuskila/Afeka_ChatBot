
import os
import sys

# Add project root to path for imports
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Load environment variables - force reload from .env file in current directory
# THIS MUST BE DONE BEFORE ANY OTHER IMPORTS THAT RELY ON .env variables
from dotenv import load_dotenv
load_dotenv(override=True)

# Set flag to indicate we're running in backend
os.environ["RUNNING_IN_BACKEND"] = "true"

from fastapi import FastAPI, HTTPException, Depends, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
from typing import Dict, Any, List, Optional
import json
import logging
from supabase import create_client, Client
from fastapi.security import APIKeyHeader
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
import time
import secrets
import re
from datetime import datetime
import uuid
import asyncio
import threading
from contextlib import asynccontextmanager
from src.backend.app.config.settings import settings

# Import vector management router
from src.backend.api.vector_management import router as vector_router

# Import RAG router
from src.backend.app.api.routes.rag import router as rag_router

# Import title generation router
from src.backend.app.api.routes.title_generation import router as title_router

# Import API Keys router
from src.backend.app.api.routes.api_keys import router as api_keys_router

# --- NEW IMPORTS FOR CHAT SERVICE ---
from src.backend.app.services.chat_service import ChatService
from src.backend.app.core.interfaces import IChatService
from src.backend.app.domain.models import ChatRequest as ChatRequestModel, ChatMessageHistoryItem # Renamed to avoid conflict with FastAPI's Request

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Print debug info if needed
if settings.GEMINI_API_KEY:
    logger.info("âœ… Gemini API key configured successfully")
else:
    logger.warning("âš ï¸  GEMINI_API_KEY not found. Ensure it is set in the .env file.")



# Rate limiting setup
API_RATE_LIMIT = int(os.environ.get("API_RATE_LIMIT", "500"))  # Requests per minute - ××•×’×“×œ ×œ×˜×¡×˜×™×
rate_limit_data = {}

# Initialize Supabase client
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY") or os.environ.get("SUPABASE_SERVICE_KEY")

# Validate Supabase configuration
if not SUPABASE_URL or not SUPABASE_KEY:
    logger.error("âŒ Missing Supabase configuration. Check SUPABASE_URL and SUPABASE_KEY environment variables.")
    raise ValueError("Supabase configuration is required")

# Initialize Supabase client with connection pooling
try:
    # Initialize without custom options for now (Supabase client format issue)
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    logger.info("âœ… Supabase client initialized successfully")
except Exception as e:
    logger.error(f"âŒ Failed to initialize Supabase client: {e}")
    raise

# Set AI Service URL
AI_SERVICE_URL = os.environ.get("AI_SERVICE_URL", "http://localhost:5000")
logger.debug(f"ğŸ¤– AI service configured at: {AI_SERVICE_URL}")

# Security - API key for internal API calls
API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)

# Generate random API key if not in environment
INTERNAL_API_KEY = os.environ.get("INTERNAL_API_KEY", secrets.token_urlsafe(32))

# Input validation pattern for document IDs
ID_PATTERN = re.compile(r"^\d+$")

# Models
class ChatRequest(BaseModel):
    message: str
    user_id: str = "anonymous"

class ChatResponse(BaseModel):
    response: Dict[str, Any]

class Document(BaseModel):
    title: str
    content: str
    category: Optional[str] = None
    tags: Optional[List[str]] = None

# ×ª×”×œ×™×š ×¢×™×‘×•×“ ×”××¡××›×™× ×”××•×˜×•××˜×™
auto_processor_thread = None

def run_document_processor():
    """×”×¤×¢×œ×ª ××¢×‘×“ ×”××¡××›×™× ×”××•×˜×•××˜×™ ×‘×ª×”×œ×™×š × ×¤×¨×“"""
    from src.ai.scripts.auto_process_documents import run_processor
    
    logger.info("Starting automatic document processor")
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    # ×”×¤×¢×œ×ª ×”××¢×‘×“ ×¢× ×‘×“×™×§×” ×›×œ 30 ×©× ×™×•×ª
    loop.run_until_complete(run_processor(interval=30))

# Lifespan context manager (replaces deprecated on_event)
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup and shutdown events"""
    global auto_processor_thread
    
    # Startup
    logger.info("ğŸš€ Starting application...")
    
    # Initialize Google Gemini API key if available
    if settings.GEMINI_API_KEY:
        import google.generativeai as genai
        genai.configure(api_key=settings.GEMINI_API_KEY)
        logger.info("âœ… Google Generative AI API key configured successfully.")
    else:
        logger.warning("âš ï¸ GEMINI_API_KEY not found. Gemini features will be limited.")
    
    # Start document processor thread
    auto_processor_thread = threading.Thread(target=run_document_processor, daemon=True)
    auto_processor_thread.start()
    logger.info("âœ… Document processor thread started")
    
    logger.info("ğŸ‰ Application startup complete!")
    
    yield  # Application runs here
    
    # Shutdown
    logger.info("ğŸ”„ Server shutting down...")
    # The processor thread will stop automatically as it's a daemon thread
    logger.info("âœ… Shutdown complete")

# Initialize FastAPI app (after lifespan is defined)
app = FastAPI(
    title="Afeka ChatBot API",
    description="Backend API for Afeka College Regulations ChatBot",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    openapi_url="/api/openapi.json",
    lifespan=lifespan  # Use the new lifespan context manager
)

# Now configure CORS and middleware AFTER app is created
ALLOWED_ORIGINS = os.environ.get(
    "ALLOWED_ORIGINS", 
    "http://localhost:5173,http://localhost:3000,http://localhost:3001,http://127.0.0.1:5173,http://127.0.0.1:3000"
).split(",")

# Only log CORS configuration in debug mode
if settings.ENVIRONMENT == "development":
    logger.debug(f"ğŸŒ Configured CORS with allowed origins: {ALLOWED_ORIGINS}")

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Security middleware
if os.environ.get("ENV", "development") == "production":
    ALLOWED_HOSTS = os.environ.get("ALLOWED_HOSTS", "localhost").split(",")
    app.add_middleware(TrustedHostMiddleware, allowed_hosts=ALLOWED_HOSTS)
    logger.info(f"ğŸ”’ Security: TrustedHostMiddleware enabled for production")

# Include routers
app.include_router(vector_router)
app.include_router(rag_router, prefix="/api/rag")
app.include_router(title_router)
app.include_router(api_keys_router)

# Add middleware after app is created
@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response

@app.middleware("http")
async def add_security_headers(request: Request, call_next):
    response = await call_next(request)
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
    response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    return response

@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    # Get client IP
    client_ip = request.client.host if request.client else "unknown"
    
    # Skip rate limiting for certain paths
    if request.url.path in ["/api/health", "/", "/api/docs", "/api/redoc", "/api/openapi.json"]:
        return await call_next(request)
    
    current_time = time.time()
    minute_window = int(current_time / 60)
    
    # Initialize or reset rate limit data for new minute window
    if client_ip not in rate_limit_data or rate_limit_data[client_ip]["window"] != minute_window:
        rate_limit_data[client_ip] = {"window": minute_window, "count": 0}
    
    # Increment request count for this IP
    rate_limit_data[client_ip]["count"] += 1
    
    # Check if rate limit exceeded
    if rate_limit_data[client_ip]["count"] > API_RATE_LIMIT:
        logger.warning(f"Rate limit exceeded for IP {client_ip}")
        return JSONResponse(
            status_code=429,
            content={"error": "Too many requests. Please try again later."}
        )
    
    # Add rate limit headers
    response = await call_next(request)
    response.headers["X-RateLimit-Limit"] = str(API_RATE_LIMIT)
    response.headers["X-RateLimit-Remaining"] = str(max(0, API_RATE_LIMIT - rate_limit_data[client_ip]["count"]))
    response.headers["X-RateLimit-Reset"] = str((minute_window + 1) * 60)
    
    return response

@app.get("/")
async def root():
    """Root endpoint that confirms the API is running"""
    return {"message": "Afeka ChatBot API is running"}

@app.get("/api/health")
async def health_check():
    """Enhanced health check endpoint for monitoring"""
    import time
    start_time = time.time()
    
    health_status = {
        "status": "ok",
        "timestamp": time.time(),
        "services": {}
    }
    
    # Check Supabase connection
    try:
        # Quick DB test - check if we can query an existing table
        result = supabase.table('api_keys').select('id').limit(1).execute()
        health_status["services"]["supabase"] = {
            "status": "connected",
            "response_time_ms": round((time.time() - start_time) * 1000, 2)
        }
    except Exception as e:
        health_status["services"]["supabase"] = {
            "status": "disconnected",
            "error": str(e)[:100] + "..." if len(str(e)) > 100 else str(e)
        }
        health_status["status"] = "degraded"
    
    # Check Key Manager
    try:
        from src.ai.core.database_key_manager import DatabaseKeyManager
        key_manager = DatabaseKeyManager(use_direct_supabase=True)
        # Quick key count check
        await key_manager.refresh_keys()
        key_count = len(key_manager.api_keys)
        health_status["services"]["key_manager"] = {
            "status": "ok",
            "active_keys": key_count
        }
    except Exception as e:
        health_status["services"]["key_manager"] = {
            "status": "error",
            "error": str(e)[:100] + "..." if len(str(e)) > 100 else str(e)
        }
        health_status["status"] = "degraded"
    
    # Overall response time
    health_status["response_time_ms"] = round((time.time() - start_time) * 1000, 2)
    
    return health_status

# Singleton instance of ChatService
_chat_service_instance = None

def get_chat_service() -> IChatService:
    """
    Return a singleton instance of ChatService.
    This ensures the RAG service is initialized only once and reused across requests.
    """
    global _chat_service_instance
    
    if _chat_service_instance is None:
        logger.info("Initializing ChatService singleton instance")
        _chat_service_instance = ChatService()
    
    return _chat_service_instance

# --- MODIFIED /api/chat ENDPOINT ---
@app.post("/api/chat")
async def chat(
    chat_request_data: ChatRequestModel, # Use the Pydantic model for request body
    chat_service: IChatService = Depends(get_chat_service) # Dependency inject ChatService
):
    """
    Process chat messages using the injected ChatService (LangChain based).
    """
    try:
        logger.info(f"Received chat request via ChatService. User: {chat_request_data.user_id}, Message: {chat_request_data.message[:50]}...")
        if chat_request_data.history:
            logger.info(f"History provided with {len(chat_request_data.history)} items.")

        # Validate message length (can also be done in Pydantic model)
        if len(chat_request_data.message) > settings.MAX_CHAT_MESSAGE_LENGTH:
            raise HTTPException(status_code=400, detail=f"Message too long (max {settings.MAX_CHAT_MESSAGE_LENGTH} characters)")

        # Process the message through the injected chat service
        # The history from chat_request_data.history (List[ChatMessageHistoryItem])
        # is directly compatible with what ChatService.process_chat_message expects.
        ai_response_data = await chat_service.process_chat_message(
            user_message=chat_request_data.message,
            user_id=chat_request_data.user_id,
            history=chat_request_data.history 
        )
        
        # ai_response_data should be like {"response": "actual AI text"}
        logger.info(f"Response from ChatService: {str(ai_response_data)[:100]}...")
                
        # Return the response from the service. 
        # Ensure it's in a format the frontend expects.
        # If ai_response_data is already {"response": "text"}, it's fine.
        # If it's just the text, wrap it: return {"response": ai_response_data}
        # Based on ChatService, it returns Dict[str, Any], likely {"response": "..."}
        return JSONResponse(
            content=ai_response_data,
            headers={"Content-Type": "application/json; charset=utf-8"}
        )
            
    except HTTPException as he:
        # Re-raise HTTPExceptions to be handled by FastAPI's exception handler
        raise he
    except json.JSONDecodeError as e: # Should be less likely if Pydantic handles parsing
        logger.error(f"Invalid JSON in request (should be caught by Pydantic): {e}")
        return JSONResponse(
            status_code=400,
            content={"error": "Invalid JSON in request body", "message": "Error processing your request"},
            headers={"Content-Type": "application/json; charset=utf-8"}
        )
    except Exception as e:
        import traceback
        logger.error(f"Unexpected error in new chat endpoint: {e}")
        logger.error(traceback.format_exc())
        return JSONResponse(
            status_code=500,
            # Provide a more generic error to the client for non-HTTP exceptions
            content={"error": "Internal server error processing chat request"}, 
            headers={"Content-Type": "application/json; charset=utf-8"}
        )

# --- NEW DEMO CHAT ENDPOINT (NO AUTH REQUIRED) ---
@app.post("/api/chat/demo")
async def chat_demo(request: Dict[str, Any]):
    """
    Demo chat endpoint that uses RAG directly without authentication.
    This is for users not logged in who want to test the system.
    """
    try:
        query = request.get("query", "").strip()
        
        if not query:
            raise HTTPException(status_code=400, detail="Query cannot be empty")
        
        logger.info(f"Received demo chat request: {query[:50]}...")
        
        # Import RAGService here to avoid circular imports
        try:
            from src.ai.services.rag_service import RAGService
            
            # Create RAG service with current profile
            rag_service = RAGService()
            
            # Run the query through RAG - same as Test Center
            result = await rag_service.generate_answer(query, search_method="hybrid")
            
            # Get chunk text for display
            chunk_text = ""
            chunks_selected = result.get("chunks_selected", [])
            if chunks_selected:
                # Take the first chunk as the main chunk text
                first_chunk = chunks_selected[0]
                chunk_text = first_chunk.get("chunk_text", first_chunk.get("content", ""))
            
            # Return in the format expected by Chat Preview
            return JSONResponse(
                content={
                    "answer": result.get("answer", "No answer generated"),
                    "response": result.get("answer", "No answer generated"), # Both formats for compatibility
                    "responseTime": result.get("response_time_ms", 0),
                    "sourcesFound": len(result.get("sources", [])),
                    "chunks": len(result.get("chunks_selected", [])),
                    "searchMethod": result.get("search_method", "hybrid"),
                    "chunkText": chunk_text
                }
            )
            
        except ImportError as import_err:
            logger.error(f"Could not import RAGService for demo: {import_err}")
            # Fallback to simple Gemini response
            return JSONResponse(
                content={
                    "answer": "×©×œ×•×! ××¦×˜×¢×¨, ×©×™×¨×•×ª ×”-RAG ××™× ×• ×–××™×Ÿ ×›×¨×’×¢. ×”×× ×ª×•×›×œ ×œ× ×¡×— ××ª ×©××œ×ª×š ×©×•×‘?",
                    "response": "×©×œ×•×! ××¦×˜×¢×¨, ×©×™×¨×•×ª ×”-RAG ××™× ×• ×–××™×Ÿ ×›×¨×’×¢. ×”×× ×ª×•×›×œ ×œ× ×¡×— ××ª ×©××œ×ª×š ×©×•×‘?"
                }
            )
            
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.exception(f"Error in demo chat endpoint: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "error": "Internal server error",
                "answer": "××¦×˜×¢×¨, ××™×¨×¢×” ×©×’×™××” ×‘××¢×¨×›×ª. ×× × × ×¡×” ×©×•×‘.",
                "response": "××¦×˜×¢×¨, ××™×¨×¢×” ×©×’×™××” ×‘××¢×¨×›×ª. ×× × × ×¡×” ×©×•×‘."
            }
        )

@app.get("/api/documents")
async def get_documents():
    """
    Retrieve available documents from Supabase storage
    
    Returns a list of documents with their metadata
    """
    try:
        if not supabase:
            logger.warning("Supabase connection not available - returning mock data")
            # Return mock data instead of failing when Supabase is not available
            return [
                {
                    "id": 1,
                    "title": "Academic Regulations",
                    "content": "This document contains the academic regulations for Afeka College",
                    "category": "Academic",
                    "created_at": "2024-01-01T00:00:00"
                },
                {
                    "id": 2,
                    "title": "Student Handbook",
                    "content": "A comprehensive guide for Afeka College students",
                    "category": "Student",
                    "created_at": "2024-01-15T00:00:00"
                },
                {
                    "id": 3, 
                    "title": "Course Catalog",
                    "content": "List of all courses offered at Afeka College",
                    "category": "Academic",
                    "created_at": "2024-02-01T00:00:00"
                }
            ]
        
        # Query documents from Supabase
        response = supabase.table('documents').select('*').execute()
        
        if hasattr(response, 'data'):
            logger.debug(f"ğŸ“Š Retrieved {len(response.data)} documents from database")
            return response.data
        else:
            logger.warning("âŒ No data returned from Supabase query")
            return []
            
    except Exception as e:
        logger.error(f"Error retrieving documents: {str(e)}")
        # Instead of failing with 500, return mock data
        logger.warning("Returning mock data due to error")
        return [
            {
                "id": 1,
                "title": "Academic Regulations",
                "content": "This document contains the academic regulations for Afeka College",
                "category": "Academic",
                "created_at": "2024-01-01T00:00:00"
            },
            {
                "id": 2,
                "title": "Student Handbook",
                "content": "A comprehensive guide for Afeka College students",
                "category": "Student",
                "created_at": "2024-01-15T00:00:00"
            }
        ]

@app.get("/api/documents/{document_id}")
async def get_document(document_id: str):
    """
    Retrieve a specific document by ID
    
    Args:
        document_id: The ID of the document to retrieve
        
    Returns:
        Document data including URL for download
    """
    try:
        # Validate document_id format for security
        if not ID_PATTERN.match(document_id):
            raise HTTPException(status_code=400, detail="Invalid document ID format")
            
        doc_id = int(document_id)
        
        if not supabase:
            logger.warning(f"Supabase connection not available - returning mock data for document ID {doc_id}")
            # Return mock data for the specific document ID
            mock_documents = {
                1: {
                    "id": 1,
                    "title": "Academic Regulations",
                    "content": "This document contains the academic regulations for Afeka College",
                    "category": "Academic",
                    "created_at": "2024-01-01T00:00:00"
                },
                2: {
                    "id": 2,
                    "title": "Student Handbook",
                    "content": "A comprehensive guide for Afeka College students",
                    "category": "Student",
                    "created_at": "2024-01-15T00:00:00"
                },
                3: {
                    "id": 3, 
                    "title": "Course Catalog",
                    "content": "List of all courses offered at Afeka College",
                    "category": "Academic",
                    "created_at": "2024-02-01T00:00:00"
                }
            }
            
            if doc_id in mock_documents:
                return mock_documents[doc_id]
            else:
                # Create a generic mock document for any other ID
                return {
                    "id": doc_id,
                    "title": f"Document {doc_id}",
                    "content": f"This is a mock document with ID {doc_id}",
                    "category": "General",
                    "created_at": "2024-01-01T00:00:00"
                }
        
        # Query the specific document
        response = supabase.table('documents').select('*').eq('id', doc_id).execute()
        
        if not response.data or len(response.data) == 0:
            logger.warning(f"Document with ID {doc_id} not found")
            
            # Return a mock document instead of 404 error
            return {
                "id": doc_id,
                "title": f"Document {doc_id}",
                "content": f"This is a mock document with ID {doc_id}",
                "category": "General",
                "created_at": "2024-01-01T00:00:00",
                "note": "This is a mock document because the original was not found"
            }
            
        logger.debug(f"ğŸ“„ Retrieved document with ID {doc_id}")
        return response.data[0]
        
    except HTTPException as he:
        # Only re-raise HTTP exceptions for invalid format
        if he.status_code == 400:
            raise he
        logger.error(f"HTTP Exception: {str(he)}")
        # Return mock data for other HTTP exceptions
        return {"id": document_id, "title": "Error Document", "content": "This document could not be found", "error": str(he)}
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid document ID format")
    except Exception as e:
        logger.error(f"Error retrieving document {document_id}: {str(e)}")
        # Return mock data instead of failing
        return {
            "id": int(document_id) if document_id.isdigit() else 0,
            "title": "Error Document",
            "content": "There was an error retrieving this document",
            "category": "Error",
            "created_at": "2024-01-01T00:00:00",
            "error_message": str(e)
        }

@app.post("/api/documents", status_code=201)
async def create_document(document: Document, api_key: str = Depends(api_key_header)):
    """
    Create a new document in Supabase
    
    Requires API key for authentication
    """
    # Validate API key
    if api_key != INTERNAL_API_KEY:
        raise HTTPException(
            status_code=401,
            detail="Invalid API Key",
            headers={"WWW-Authenticate": "APIKey"}
        )
        
    try:
        if not supabase:
            raise HTTPException(status_code=503, detail="Supabase connection not available")
            
        # Validate document length
        if len(document.content) > 100000:
            raise HTTPException(status_code=400, detail="Document content too large (max 100KB)")
            
        # Create document in Supabase
        doc_data = document.model_dump()
        doc_data["created_at"] = "now()"
        
        response = supabase.table('documents').insert(doc_data).execute()
            
        if not response.data:
            raise HTTPException(status_code=500, detail="Failed to create document")
            
        logger.info(f"ğŸ“ Created new document: {document.title}")
        return {"success": True, "id": response.data[0]["id"], "message": "Document created successfully"}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating document: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to create document")

# Error handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content={"error": exc.detail}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unhandled exception: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={"error": "Internal server error"}
    )

# Proxy routes for Supabase
@app.post("/api/proxy/chat_sessions")
async def proxy_create_chat_session(request: Request):
    """Proxy endpoint for creating chat sessions"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        body = await request.json()
        logger.debug(f"ğŸ’¬ Creating chat session for user: {body.get('user_id', 'unknown')}")
        
        # Check if chat_sessions table exists
        try:
            test_query = supabase.table("chat_sessions").select("count").limit(1).execute()
            
            # Proceed with normal insert
            result = supabase.table("chat_sessions").insert(body).execute()
            
            # Also create a corresponding conversation record if conversations table exists
            try:
                if result.data:
                    # Check if conversations table exists
                    conversation_test = supabase.table("conversations").select("count").limit(1).execute()
                    
                    conversation_data = {
                        "conversation_id": result.data[0]["id"],
                        "user_id": body.get("user_id"),
                        "title": body.get("title"),
                        "created_at": body.get("created_at", result.data[0].get("created_at")),
                        "updated_at": body.get("updated_at", result.data[0].get("updated_at")),
                        "is_active": True
                    }
                    
                    supabase.table("conversations").insert(conversation_data).execute()
            except Exception as conv_err:
                logger.warning(f"Couldn't create conversation record: {conv_err}")
                
            return JSONResponse(content=result.data)
            
        except Exception as table_err:
            logger.warning(f"chat_sessions table may not exist: {table_err}")
            # Generate a mock uuid for the session
            session_id = str(uuid.uuid4())
            
            # Return a mock successful response with session data
            mock_response = [{
                "id": session_id,
                "user_id": body.get('user_id', 'unknown'),
                "title": body.get('title', 'New Chat'),
                "created_at": body.get('created_at', datetime.now().isoformat()),
                "updated_at": body.get('updated_at', datetime.now().isoformat())
            }]
            logger.info(f"Returning mock session response with ID: {session_id}")
            return JSONResponse(content=mock_response)
    except Exception as e:
        logger.error(f"Error proxying chat session creation: {e}")
        # Create a mock response instead of error
        session_id = str(uuid.uuid4())
        mock_response = [{
            "id": session_id,
            "user_id": "unknown",
            "title": "New Chat",
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }]
        return JSONResponse(content=mock_response)

@app.get("/api/proxy/chat_sessions")
async def proxy_get_chat_sessions(user_id: str, request: Request):
    """Proxy endpoint for getting chat sessions"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        logger.debug(f"ğŸ“‹ Fetching chat sessions for user: {user_id}")
        
        # First check if the chat_sessions table exists
        try:
            # Try to get table info or do a minimal query
            test_query = supabase.table("chat_sessions").select("count").limit(1).execute()
        except Exception as table_err:
            # Table likely doesn't exist - return empty array instead of error
            logger.warning(f"chat_sessions table may not exist: {table_err}")
            logger.debug("âŒ Returning empty array due to missing table")
            return JSONResponse(content=[])
        
        # Execute the actual query
        try:
            result = supabase.table("chat_sessions").select("*").eq("user_id", user_id).order("updated_at", desc=True).execute()
            logger.debug(f"ğŸ“Š Found {len(result.data) if hasattr(result, 'data') else 0} chat sessions")
            return JSONResponse(content=result.data)
        except Exception as query_err:
            logger.error(f"Query execution error: {query_err}")
            # Return empty array instead of error
            return JSONResponse(content=[])
    except Exception as e:
        logger.error(f"Error proxying chat sessions retrieval: {e}")
        # Return empty array instead of error
        return JSONResponse(content=[])

@app.get("/api/proxy/chat_sessions/{session_id}")
async def proxy_get_chat_session(session_id: str, request: Request):
    """Proxy endpoint for getting a specific chat session"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        logger.debug(f"ğŸ” Fetching chat session with ID: {session_id}")
        
        # Check if tables exist first
        try:
            test_query = supabase.table("chat_sessions").select("count").limit(1).execute()
        except Exception as table_err:
            logger.warning(f"chat_sessions table may not exist: {table_err}")
            # Return empty session with messages array
            return JSONResponse(content={"id": session_id, "messages": []})
            
        # Get the session
        try:
            session_result = supabase.table("chat_sessions").select("*").eq("id", session_id).single().execute()
            
            if not session_result.data:
                logger.warning(f"Chat session with ID {session_id} not found")
                return JSONResponse(content={"id": session_id, "messages": []})
                
            # Check if messages table exists
            try:
                messages_result = supabase.table("messages").select("*").eq("conversation_id", session_id).order("created_at").execute()
                messages = messages_result.data or []
            except Exception as msg_err:
                logger.warning(f"messages table may not exist: {msg_err}")
                messages = []
                
            # Combine the data
            result = {
                **session_result.data,
                "messages": messages
            }
            
            return JSONResponse(content=result)
        except Exception as query_err:
            logger.error(f"Error fetching session: {query_err}")
            return JSONResponse(content={"id": session_id, "messages": []})
    except Exception as e:
        logger.error(f"Error proxying chat session retrieval: {e}")
        return JSONResponse(content={"id": session_id, "messages": []})

@app.post("/api/proxy/messages")
async def proxy_create_message(request: Request):
    """Proxy endpoint for creating messages"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        body = await request.json()
        logger.debug(f"ğŸ’¬ Creating message for conversation: {body.get('conversation_id', 'unknown')}")
        
        # Check if messages table exists
        try:
            test_query = supabase.table("messages").select("count").limit(1).execute()
            
            # Proceed with normal insert
            result = supabase.table("messages").insert(body).execute()
            return JSONResponse(content=result.data)
        except Exception as table_err:
            logger.warning(f"messages table may not exist: {table_err}")
            # Return a mock successful response with an ID
            mock_id = body.get('id', f"mock-{int(time.time())}")
            mock_response = [{
                "id": mock_id,
                "conversation_id": body.get('conversation_id', 'unknown'),
                "created_at": body.get('created_at', datetime.now().isoformat()),
                "message_text": body.get('message_text', '')
            }]
            logger.debug(f"ğŸ­ Returning mock message response with ID: {mock_id}")
            return JSONResponse(content=mock_response)
    except Exception as e:
        logger.error(f"Error proxying message creation: {e}")
        # Create a mock response instead of error
        mock_id = f"mock-{int(time.time())}"
        mock_response = [{
            "id": mock_id,
            "created_at": datetime.now().isoformat()
        }]
        return JSONResponse(content=mock_response)

@app.get("/api/proxy/documents")
async def proxy_get_documents(request: Request):
    """Proxy endpoint for retrieving documents"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        result = supabase.table("documents").select("*").order("created_at", desc=True).execute()
        return JSONResponse(content=result.data)
    except Exception as e:
        logger.error(f"Error proxying documents retrieval: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/proxy/documents/{document_id}")
async def proxy_get_document(document_id: int, request: Request):
    """Proxy endpoint for retrieving a specific document"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        result = supabase.table("documents").select("*").eq("id", document_id).single().execute()
        if not result.data:
            raise HTTPException(status_code=404, detail="Document not found")
        return JSONResponse(content=result.data)
    except Exception as e:
        logger.error(f"Error proxying document retrieval: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/proxy/documents")
async def proxy_create_document(request: Request):
    """Proxy endpoint for creating documents"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        body = await request.json()
        # Use RPC function if it exists, otherwise fall back to direct insert
        try:
            result = supabase.rpc("create_document", {
                "name": body.get("name"),
                "type": body.get("type"),
                "size": body.get("size"),
                "url": body.get("url"),
                "user_id": body.get("user_id")
            }).execute()
            
            # Get the newly created document
            if result.data:
                doc_result = supabase.table("documents").select("*").eq("id", result.data).single().execute()
                return JSONResponse(content=doc_result.data)
            else:
                raise HTTPException(status_code=500, detail="Document creation failed")
        except Exception as rpc_err:
            logger.warning(f"RPC method failed, falling back to direct insert: {rpc_err}")
            result = supabase.table("documents").insert(body).execute()
            return JSONResponse(content=result.data[0] if result.data else None)
    except Exception as e:
        logger.error(f"Error proxying document creation: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/proxy/documents/{document_id}")
async def proxy_delete_document(document_id: int, request: Request):
    """Proxy endpoint for deleting documents"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        # Get document info before deletion - use safer approach that won't fail if no results
        doc_result = supabase.table("documents").select("*").eq("id", document_id).execute()
        if not doc_result.data or len(doc_result.data) == 0:
            # Document already deleted or doesn't exist - return success
            return JSONResponse(content={"success": True, "message": "Document not found or already deleted"})
        
        # ×—×©×•×‘ ×œ××—×•×§ ××ª ×”-embeddings ×œ×¤× ×™ ×©××•×—×§×™× ××ª document_chunks ×›×“×™ ×œ×× ×•×¢ ××¦×‘ ×©×‘×•
        # ×™×© embeddings ×œ×œ× chunks ××§×•×©×¨×™×
        try:
            # Get chunk IDs first
            chunk_result = supabase.table("document_chunks").select("id, embedding_id").eq("document_id", document_id).execute()
            
            if chunk_result.data and len(chunk_result.data) > 0:
                # Extract embedding IDs
                embedding_ids = [chunk.get("embedding_id") for chunk in chunk_result.data if chunk.get("embedding_id")]
                
                # Delete embeddings if we have any
                if embedding_ids:
                    logger.info(f"Deleting {len(embedding_ids)} embeddings for document {document_id}")
                    for embedding_id in embedding_ids:
                        supabase.table("embeddings").delete().eq("id", embedding_id).execute()
            
            # Delete document chunks
            logger.info(f"Deleting document chunks for document {document_id}")
            supabase.table("document_chunks").delete().eq("document_id", document_id).execute()
        except Exception as chunks_err:
            logger.warning(f"Error deleting document chunks or embeddings: {chunks_err}")
            
        # Delete from database
        result = supabase.table("documents").delete().eq("id", document_id).execute()
        
        # Try to delete from storage if URL exists
        if doc_result.data and len(doc_result.data) > 0 and doc_result.data[0].get("url"):
            try:
                url = doc_result.data[0].get("url")
                
                # Extract storage path from URL
                # This is a simplified version - we might need more complex logic based on URL format
                parts = url.split("/")
                filename = parts[-1].split("?")[0]
                
                # Try to delete the file - silently fail if it doesn't work
                supabase.storage.from_("documents").remove([filename])
            except Exception as storage_err:
                logger.warning(f"Failed to delete file from storage: {storage_err}")
                
        return JSONResponse(content={"success": True})
    except Exception as e:
        logger.error(f"Error proxying document deletion: {e}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "error": "Internal server error", "message": str(e)}
        )

@app.patch("/api/proxy/chat_session/{session_id}")
async def proxy_update_chat_session(session_id: str, request: Request):
    """Proxy endpoint for updating a chat session"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        body = await request.json()
        
        # Ensure we have the updated timestamp
        if 'updated_at' not in body:
            body['updated_at'] = datetime.now().isoformat()
            
        result = supabase.table("chat_sessions").update(body).eq("id", session_id).execute()
        
        # Also update conversation if it exists
        try:
            conversation_update = {
                'updated_at': body.get('updated_at'),
                'last_message_at': body.get('updated_at')
            }
            if 'title' in body:
                conversation_update['title'] = body['title']
                
            supabase.table("conversations").update(conversation_update).eq("conversation_id", session_id).execute()
        except Exception as conv_err:
            logger.warning(f"Error updating conversation: {conv_err}")
        
        return JSONResponse(content={"success": True, "data": result.data})
    except Exception as e:
        logger.error(f"Error proxying chat session update: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.patch("/api/proxy/chat_sessions/{session_id}")
async def proxy_update_chat_sessions_plural(session_id: str, request: Request):
    """Alias for proxy_update_chat_session to maintain backwards compatibility"""
    return await proxy_update_chat_session(session_id, request)

@app.get("/api/proxy/messages_schema")
async def proxy_get_messages_schema(request: Request):
    """Proxy endpoint for getting the message table schema"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        # Get a single message to determine schema
        result = supabase.table("messages").select("*").limit(1).execute()
        
        if not result.data or len(result.data) == 0:
            # Create a dummy message with all possible fields
            dummy_schema = {
                "message_id": 0,
                "conversation_id": "",
                "user_id": "",
                "request": "",
                "response": "",
                "created_at": "",
                "status": "",
                "status_updated_at": "",
                "error_message": "",
                "request_type": "",
                "request_payload": {},
                "response_payload": {},
                "status_code": 0,
                "processing_start_time": "",
                "processing_end_time": "",
                "processing_time_ms": 0,
                "is_sensitive": False,
                "metadata": {},
                "chat_session_id": ""
            }
            return JSONResponse(content={"columns": list(dummy_schema.keys())})
        
        # Return the column names
        columns = list(result.data[0].keys())
        return JSONResponse(content={"columns": columns})
    except Exception as e:
        logger.error(f"Error proxying message schema retrieval: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/proxy/message")
async def proxy_update_message(request: Request):
    """Proxy endpoint for adding a message with custom ID"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        # ×§×¨×™××ª ×’×•×£ ×”×‘×§×©×” ×•×¨×™×©×•× ×œ×•×’
        body_bytes = await request.body()
        body_str = body_bytes.decode('utf-8', errors='replace')
        logger.info(f"Raw message request body: {body_str[:200]}...")
        
        try:
            body = json.loads(body_str)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON body: {e}")
            return JSONResponse(
                status_code=400, 
                content={"error": f"Invalid JSON in request body: {str(e)}"}
            )
        
        logger.info(f"Parsed message data: {str(body)[:200]}...")
        
        # ×•×™×“×•×™ ×©×”×©×“×•×ª ×”× ×“×¨×©×™× ×§×™×™××™×
        required_fields = ["user_id", "conversation_id"]
        missing_fields = [field for field in required_fields if not body.get(field)]
        
        if missing_fields:
            logger.error(f"Missing required fields: {missing_fields}")
            return JSONResponse(
                status_code=400,
                content={"error": f"Missing required fields: {', '.join(missing_fields)}"}
            )
        
        # Generate ID if not provided
        if 'message_id' not in body and 'id' not in body:
            # Generate timestamp-based numeric ID
            import time
            import random
            body['message_id'] = int(time.time() * 1000) * 1000 + random.randint(0, 999)
        
        # Ensure created_at is set
        if 'created_at' not in body:
            body['created_at'] = datetime.now().isoformat()
            
        # × ×§×” ×©×“×•×ª null, undefined ××• ×¨×™×§×™×
        cleaned_body = {}
        for key, value in body.items():
            if value is not None and value != "" and key != "":
                # ×”××¨ ×¢×¨×›×™ JSON ×œ×¤×™ ×”×¦×•×¨×š
                if key in ['request_payload', 'response_payload', 'metadata'] and isinstance(value, dict):
                    try:
                        cleaned_body[key] = json.dumps(value)
                    except Exception as e:
                        logger.warning(f"Could not serialize JSON field {key}: {e}")
                        # ×”×©×ª××© ×‘×¢×¨×š ××§×•×¨×™ ×× ×”×”××¨×” × ×›×©×œ×ª
                        cleaned_body[key] = str(value)
                else:
                    cleaned_body[key] = value
        
        # ×•×™×“×•× ×©×”××–×”×™× ×”× ××—×¨×•×–×•×ª, ×”×¢×¨×›×™× ×©××™× × ××—×¨×•×–×•×ª ×™×•××¨×•
        if 'conversation_id' in cleaned_body and not isinstance(cleaned_body['conversation_id'], str):
            cleaned_body['conversation_id'] = str(cleaned_body['conversation_id'])
            
        if 'user_id' in cleaned_body and not isinstance(cleaned_body['user_id'], str):
            cleaned_body['user_id'] = str(cleaned_body['user_id'])
        
        logger.info(f"Cleaned message data for insert: {str(cleaned_body)[:200]}...")
        
        try:
            # × ×¡×” ×œ×”×•×¡×™×£ ××ª ×”×”×•×“×¢×” ×œ×˜×‘×œ×”
            result = supabase.table("messages").insert(cleaned_body).execute()
            
            if hasattr(result, 'data') and result.data:
                logger.info(f"Message insert success, returned data: {str(result.data)[:200]}...")
                # ×”×—×–×¨ ××ª ×”× ×ª×•× ×™× ×›×¤×™ ×©×”×•×—×–×¨×• ××¡×•×¤××‘×™×™×¡
                return JSONResponse(content=result.data[0])
            else:
                # ×× ××™×Ÿ × ×ª×•× ×™× ×‘×ª×©×•×‘×”, ×”×—×–×¨ ××ª ×”××–×”×” ×•×”×–××Ÿ ×©×™×¦×¨× ×•
                logger.warning("No data returned from message insert")
                response_data = {
                    "id": cleaned_body.get('message_id', cleaned_body.get('id', 'unknown')),
                    "created_at": cleaned_body.get('created_at', datetime.now().isoformat()),
                    "conversation_id": cleaned_body.get('conversation_id', ""),
                    "user_id": cleaned_body.get('user_id', "")
                }
                logger.info(f"Generating fallback response: {response_data}")
                return JSONResponse(content=response_data)
        except Exception as insert_err:
            # ×¨×™×©×•× ×›×œ ×”×¤×¨×˜×™× ×©×œ ×©×’×™××ª ×”×”×•×¡×¤×” ×œ×‘×¡×™×¡ ×”× ×ª×•× ×™×
            logger.error(f"Database insertion error: {insert_err}", exc_info=True)
            # ×‘×“×•×§ ×× ×–×• ×©×’×™××ª ×“×•×¤×œ×™×§×˜
            error_str = str(insert_err).lower()
            if "duplicate" in error_str or "unique constraint" in error_str:
                return JSONResponse(
                    status_code=409,
                    content={"error": "Message ID already exists", "detail": str(insert_err)}
                )
            # ×”×—×–×¨ ×©×’×™××” ××¤×•×¨×˜×ª
            return JSONResponse(
                status_code=500,
                content={"error": "Database error", "detail": str(insert_err)}
            )
            
    except Exception as e:
        # ×¨×™×©×•× ×›×œ ×”×¤×¨×˜×™× ×©×œ ×”×©×’×™××” ×”×›×œ×œ×™×ª
        logger.error(f"Error proxying message creation: {e}", exc_info=True)
        # ×”×—×–×¨ ×©×’×™××” ××•×‘× ×ª ×™×•×ª×¨ ×œ××©×ª××©
        return JSONResponse(
            status_code=500, 
            content={"error": "Internal server error", "detail": str(e)}
        )

@app.get("/api/proxy/search_chat_sessions")
async def proxy_search_chat_sessions(user_id: str, search_term: str, request: Request):
    """Proxy endpoint for searching chat sessions"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        # First search for chat sessions with matching titles
        title_result = supabase.table("chat_sessions").select("*").eq("user_id", user_id).ilike("title", f"%{search_term}%").execute()
        
        # Then search for messages with matching content in request or response fields
        message_result = supabase.table("messages").select("conversation_id").eq("user_id", user_id).or_(f"request.ilike.%{search_term}%,response.ilike.%{search_term}%").execute()
        
        # Get unique session IDs from message matches
        session_ids = []
        if message_result.data:
            session_ids = list(set([msg.get("conversation_id") for msg in message_result.data]))
        
        # If we have matching session IDs from messages, fetch those sessions
        content_match_sessions = []
        if session_ids:
            content_result = supabase.table("chat_sessions").select("*").eq("user_id", user_id).in_("id", session_ids).execute()
            content_match_sessions = content_result.data or []
        
        # Combine and deduplicate results
        all_sessions = title_result.data + content_match_sessions if title_result.data else content_match_sessions
        
        # Remove duplicates by creating a dictionary with session ID as key
        unique_sessions = {}
        for session in all_sessions:
            if session.get("id") not in unique_sessions:
                unique_sessions[session.get("id")] = session
        
        # Convert back to a list and sort by updated_at (newest first)
        result = list(unique_sessions.values())
        result.sort(key=lambda x: x.get("updated_at", x.get("created_at", "")), reverse=True)
        
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"Error searching chat sessions: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/proxy/chat_session/{session_id}")
async def proxy_delete_chat_session(session_id: str, request: Request):
    """Proxy endpoint for deleting a chat session"""
    if not supabase:
        raise HTTPException(status_code=503, detail="Supabase connection not available")
    
    try:
        # First delete all messages in the session
        messages_result = supabase.table("messages").delete().eq("conversation_id", session_id).execute()
        
        # Delete the conversation record if it exists
        try:
            conversation_result = supabase.table("conversations").delete().eq("conversation_id", session_id).execute()
        except Exception as conv_err:
            logger.warning(f"Error deleting conversation: {conv_err}")
        
        # Then delete the session itself
        session_result = supabase.table("chat_sessions").delete().eq("id", session_id).execute()
        
        return JSONResponse(content={"success": True})
    except Exception as e:
        logger.error(f"Error deleting chat session: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "8000"))
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=True) 